--------------------------------------------------------------------------------------------------------------------
Artificial Intelligence:
"It is a branch of computer science that enables the computer to learn without the explicit programming
(instructions based programming telling the computer to carry out the certain tasks through the instructions)."

Machine Learning:
"Machine learning is the branch of artificial intelligence that makes the computer learn through examples and
experiences. Model gets trained over the new data and this process is kept repeating time to time."

Deep Learning:
"It is the branch of the machine learning that works on the specific functions that sre called neurons as the neurons
are being designed to carry out the specific functionality."

Note:
Every neuron is the function but every function is not the neuron.
Example: RELU Neuron is used to find the maximum number and SIGMOID function is used to convert into the binary from.

There are the two main roles as the 
    a. Encoders (Human centric to machine centric)
        "Human language into the vector embeddings or into the numerical form to make it understandable for the 
         computer."
    b. Decoders (Machine Centric to human centric)
        "Vector embeddings into the human understandable language so that the human can understand it.
This all process is being carried out by the Large Language Model(Text based model or the adversal neural network)."
Diffusion models if there is the dealing with the images
Stable Diffusion:
Role of encoders and the decoders for the image generation as the working is being carried out on the pixel and 
the image is being cleared using this process. Image can also go back to the its original position as this process 
is being used in the security purposes for the theif detection.

Encoders and decoders works collectively to make the Transformers.


Natural Language Processing:
    a. Stemming:
        Remving the last few unnecessary chracters like car from caring to make it a meaningful one.
    b. Lemmatization:
        It removes the last few chracters by understanding the context as well for instance caring to care 
        or eat from ate.
    c. Limmitization:
        It is the more intelligent form as it understands the 1st, 2nd and 3rd form of the verbs as well.
--------------------------------------------------------------------------------------------------------------------
Types of Machine Learning:
----- 1. Supervised Learning    (Labelled Data)
    Types of Supervised Learning:
        a. Classification (Data with classes)
        b. Regression (Score based Answers/Feedbacks)
        Explaination
            a. Classification: 
                "Data is arranged into the classes for instance squares, triangles etc are already being classified
                 as the seperated form."
            b. Regression:
                "Score based answer for instance the output on the essay like asking the model what is your feedback
                 on the essay."


----- 2. Unsupervised Learning  (Unlabelled Data)
"A large record of data later arranged into a group or the cluster for the easy recognition and access.
It is used for the dimensionality reduction as the data with multiple inputs being given as the single output."

----- 3. Reinforcement Learning (Action Based Learning)
"Action based learning which is being carried out on the basis of the feedbacks as there are the three main
chracters in this type of learning which are agent, data , training. Agent is being trained by the use of the 
data through the process of training." For instance the child learns how to ride the bicycle like working as the 
agent in this scenario.
There are the two types of models
a. Training Mode (Model will be getting training)
b. Inference Mode/Prediction Mode  (Testing of the model)

--------------------------------------------------------------------------------------------------------------------
Prompt Engineering:
Prompt engineering deals with the methodology of prompt giving to the computer as it covers various aspects which are
as follow:
-Persona
-Task
-Steps to complete the tasks
-Context
-Constraints
-Goal 
-Format Output

Exploratory Data Analysis (EDA):
It is the process of exploring the data and perfoming the analysis over it.
These are the steps involved into this process
1. Data Analysis
        Data Engineers (Classifies the data)
        Data Analysis  (Gives analysis that what kind of model will be made through the certain type of data)
        Data Scientists (Does everything related to the data)

2. Prprocess the data (Understands the data):
a. Data Cleaning:
       Removing the unnecessary stuff from the data
b. Removing the duplication:
       Removing the duplication from the data as ther will be more light weight data
c. Data Transformation:
       Transforming the data from one certain form to another certain form.
--------------------------------------------------------------------------------------------------------------------
Follwinng are the steps involved into the EDA:
-Data Cleaning
-Descriptive Statistics (Summary Statistics)
            Mean
            Median
            Mode (Multi model)
            Inter Quartile Range (Q3-Q1)
            Standard deviation
            Dispersion
                Less dispersion means less training of the model so the less intelligent model will become and 
                greater dispersion means there will be the more training of the model and more intelligent will be 
                the model.
                Example
                                12         34       56        78        12      34        67         89         90       23 
                                sorting
                                12         12       23        34        34      56        67         78         89       90
                                min                                     34+56/2= 45                                      max
                                                                          Q2 = 45
                                           12+23/2=17.5                                               78+89/2=83.5
                                           Q1 = 18                                                    Q3 = 84

                                IQR = Q3 - Q1
                                    = 84 - 18
                                    = 66

-Data Visualization
            Data will be made visualizable
-Data Distribution
-Correlation Analysis
            Relationships of the features of the dataframe among each other and how they are depended on each other.
            It can be positive (strong) or it can be negative (weak) relationships as well.
-Outlier detection
            Outlier from the given range is detected and gets ignored as the outlier changes the direction in the 
            Average getting process.
-Data Transformation:
            Data is transformed from one place into the other one.

A coloumn can have different names in the form of:
Feature  (ML)
Attribute  (Database)
Variable (Statistics)
--------------------------------------------------------------------------------------------------------------------

Classification:
Classification is a machine learning tecnology used to predict the categorical data or discrete target variables.
It is one of the type of supervised learning which is carries out usiing the input/output pairs as the complete 
process is supervised.Models are also trained using the Input/Output pairs.
In classification the response will always be nominal/categorical/label data.
In classification, the classification name is classification label.
Types of Classification:
1. Binary Classification
    There can only be two states at one time either it is yes(1) or it is no(0).Now here the values are numeric but 
    we know the numeric answer/output is possible only in the regression and here 0,1 are numeric although it is in
    classification. So these numeric numbers are just for the representation, numeric operations cannot be performed
    on them.
2. Multi-class Classification
    More than two classes at one time.
Target Variables:
It is the variable we want to predict.
It represents the outcome or label we are interested in.
EDA helps us to find the target variable.
--------------------------------------------------------------------------------------------------------------------
There are two types of the variables which are as follow:
Dependent                               Independent
values depends upon                     Values does not depends upon 
other features                          other features.
EDA is done in the data engineering and it is done before providing the data to the ML engineer for the
machine learning purposes like the model training.
Regression:
It is a statistical technique used to model the relationship between a dependant variable and one or more
independent variables.
Dependent variables are said as "outcome" or "target".
Independent variables are said as "variables" or "features".
                                                Differences
Regression                                                         Correlation
-Predicts the values of dependant                |       -Access the strength and direction of the
variable based on independent                    |       linear relationship between variables.
variables.                                       |
-Predictive and Exploratory Modelling            |       -Descriptive analysis
-Provides the values which represents            |       -Gives a single-value (corelation-efficient)
the effect.                                      |        representing the strenght of corelation.
--------------------------------------------------------------------------------------------------------------------
Logistic Regression(classification):
It is a type of analysis for predicting the binary outcomes (two categories) 
For example: predicting whether a student will pass (1) or the student will fail(0).
having some sort of probability.
Graph:
                              |
                            1 |__________________******_______
      Dependent               |
        Varibles              |
                          0.5 |---------------*-------------   Threshold        Actually there is a s type line on the
                              |                                                 graph too
                              |
                          0   |__*******_______________________
                                  Independent variables       x

It gives two numbers (0,1) in binary.Regression gives the value which gives the insights.
--------------------------------------------------------------------------------------------------------------------

Linear Regression:
It is widely used in various fields , including finance, economics, and data sciences.Great for making the predictions 
and understanding the relationship between variables.
Score whcich is based on the lines.
Graph:
                              |             *-
                            1 |     -  -   *-
      Dependent               |    -  ---*-
        Varibles              |    -  -*-
                          0.5 |    -*- -                                       Actually there is a s slope line on the
                              |   *  -----                                          graph having multiples around it.
                              | *
                          0   |*_________________________
                                  Independent variables       x
We have to choose the line which must be close to the datapoints.Linear regression model chooses that line as well.
Line/Actual line/Ideal Line  is like  y =mx +c  (Slope intercept form)
Y: Dependant variable (Output)
x: InDependant variable (Input/feature)
m: Slope of line (how much y changes when x is 0)
c: the y intercept (the value of y when x is 0)

Loss/Error:
How far the line is from the datapoints
Actually when the error is in negative we have to extract the positive out of it to take the positive out of errors.
Mean Squared Error(MSE):
It ia common evaluation metric used in the regression task. It measures the average squared difference between the 
actual line and predicted line.
As MSE increases the Line Increases
As MSE decreases the Line decreases
Mean Absolute Error:
Taking the absolute value from the complete value 
for example : 40 from -40

Dimensionality Reduction:
Data trimming/ Removing the irrelevant information.
If dataset have 12 cols it means that the dataset have 12 dimensions
--------------------------------------------------------------------------------------------------------------------
Scikit-learn:
It is the popular python library used for the classification, regression and clustering aswell.

Data Splitting
Data is splitted into two different categories which are as:
Training Data
Testing Data

--------------------------------------------------------------------------------------------------------------------
Evaluation metrics for different models:
If the model is giving too much big MSE error, what could be the problems?
Data preprocessing falults (might be)
feature engineering required  (might be)
One thing is sure, error will stay forever, the problem is how to mitigate (minimize) this error.
MSE is directly porportional to the models performance.
A plane has the 2d or 3d graph (graph between the dependent and independent variables)
(most of the times the dependent variable is one while the independent variables are more then one.)
Data Preprocessing:
Data reduction
Data Transformation
Now, this is the main problem for the models to give the mse value more like there exists many features whose values 
are in different ranges which are need to be in the same range for the better performance of the model.We have
some standards to bring them in the same range which are as follow:
MinMax Scaler
ZeroScale/Standard Scaler
Standardization:
it is the type of data preprocessing that makes the different features have a similar scale. like in (0,1 range)

Differnt Regression Techniques:
1. Decision Tree Regression:
It involves the partitioning of the data into subsets based on the values of the independent variables and predict
the average of the target variable.
for example you have 100 different records of the players and you have given these records to the 5 categories
of differnt people (0-25), (26,50), (51,75), (76,100). Now you can get the score of the individual player easily
by calling the group which have the record of that particular player.
Like it is in this pattern:
                                         0
                                       *  *
                                      *     *
                                    *         *
                                    0          0
                                  *   *       *  *
                                *       *    *    *
                                0        0   0      0

2. Random forest regressor:
it is the learning method that creates the multiple decision trees during training and outputs the average 
prediction (for regression) from all individual trees
for example: Suppose you are in a sitting of 10 differt people and you have to make the decision on one particular
thing then you have 10 differnt people for the advice purposes and there advice also acts as the vote
in the form of pattern it would be like:
                                        x
                                       *  *
                                      *     *
                                    *         *
                                  Tree       Tree   upto tree(n)
                                    *          *            *
                                     *        *            *
                                       *     *            *
                                         *  *           *
                                           +
                                           y(outcome/result)

3. Gradient Boosting Regressors:
it is the extension of decision trees.
gradient boosting is learning techniques hat builds multiple decision trees sequentially each one correcting
the error of predecessors
for example: you have a sitting of 10 people and every person gives you an advice to move on and you act upon the 
advises and if you make a fault after following up the advise then once again you take up the advise and act upon it
and this process goes on.
-------------------------------------------------------------------------------------------------------------------
Further Explaination of Evaluation Metrics:
Entropy:
Measure of impurity.
for example: we have the three containers which have the eggs and candies in them like:
Container 01                       Container 02                     Container 03
| eccececec |                     | eeeeeeeee |                     | eeeeeeeee |
| ecececcec |                     | ccccccccc |                     | eeeeeeeee |
| ecececeec |                     | eeeeeeeee |                     | eeeeeeeee |
|-----------|                     |-----------|                     |-----------|
More Impurity                     Less Impurity                      No Impurity
More Entropy                      Less Entropy                       No Entropy
Entropy is used to find the target variable, as we want that feature as the target variable whose entropy is less
means that it must have less impurity into it. 
In the decision tree, there is the decision going on every node, in fact the decision is represented by the node
On the leaf, there is the only single class.
Decision tree is the computation friendly.
It can also work with the categorical data although other algorithms mostly likes to work with the other form of
data (mostly numeric).

Decision Tree Classifiers:
it makes the decision by splitting the data into a smaller subsets based on the values of input features
ultimately assigned a class label to each data point.It constructs a tree like data structure of decision rules 
that can be used for prediction and is easy to interpret.

Random Forest Classifier:
It is the ensemble learning method that creates the multiple decision trees during training and outputs the 
average prediction from all the individual trees.
It will have different root nodes as different target variables(can be multiple as the multiple trees are 
in the forest with different feature as the target variable)

Ensemble: 
Ensemble learning is a machine learning technique that combines the predictions generalizations. Instead of relying 
on a single model ensemble methods create a "commitee" of models and aggregate their predictions to make a final 
decision.

Min/Max Scaling:
It is a data normalization technique used to transform the data into a specific range.
formula :       x' = x- min(x)/ max(x) - min(x)   (1-0)
it saves the computational power as well.
For example:        Normalize the data [200,300,400,600,1000]
                    min(x)  = 200           max(x)  = 1000
                    scaled(x)  = (200-200)/(100-200)* (1-0)
                    = 0
                    same like this the complete form would be like
                    [0, 0.125, 0.25, 0.5, 1]

Z-Score Scaling:
It is also knowns as the standardization, is a technique used to transform the data into a standard normal 
Distribution.
formula :             Z=   score  - mean / standard deviation
For example:        Normalize the data [200,300,400,600,1000]
                    min(x)  = 200           max(x)  = 1000
                    mean  = 50
                    standard deviation = 286.4789
                    now the transformed dataset will be like
                    [-1.047, 0.698, -0,349, 0.349, 1.747]
Now we have an exampele as 
Suppose a dataset like: (have 4 features)
Business              Competetion             Values                Profit
Freelancing           Low                     Low                   Yes
E-Commerce            Medium                  Medium                No
E-Commerce            High                    High                  No
Software house        Low                     Medium                Yes
Freelancing           High                    High                  yes
Software house        Low                     Medium                no

Now we have to make the decision tree from it as well like
--------------------------------------------------------------------------------------------------------------------
Evaluation Metrics for the models:
This all the following evaluation will be only done on the binary classification models only.
There exists different evaluation metric for the models to verify whether they are performing or not.
1. Accuracy:
It is applied only on the classification, not applicable on the regression.
It is the ratio of number of correct predictions to the total number of input predictions.
formula:                  Accuracy  = Number of correct predictions
                                      -----------------------------
                                      Total number of predictions
It works only if there are equal number of samples belonging to each class.

Class Imbalance:
It is the imbalance of the data between two classes as:

                |   -----   
                |   |   |
                |   |   |
                |   |   |   
                |   |   |   ----
                |   |   |   |  |
                ---------------------------
                    90%     10%

                                              Differnce
            Imbalance                          |                         Skewness
            it is only found in the            |                      It is found only in the numerical
            categorical data                   |                       data.

Data Balance:
If the classes have the balance like 50% - 50% 
or 60% - 40% is also acceptable.
Accuracy does not always gives the good model check it sometimes confuses us by giving the higher values but 
actualy the model is not trained good. so we have to use some other metrics as well to check the model.
Evaluation Metrics:
1. Confusion Metrics:
it gives us a metrix as the output and describes the complete performance of the model.
These are the following terminologies:
  a. True Positives:
  If the predicted value is Yes, we were also interseted in Yes and the actual value was also Yes 
  then it is true positive.
  for example: 
  If we have given the model a picture of cat and we are interested in the output of the cat as well then
  the model also gives the picture of cat then it is true positives.
  b. True Negatves:
  If the predicted value is No, we were also interseted in No and the actual value was also No 
  then it is true negatives.
  for example: 
  If we have given the model a picture of non-cat and we are interested in the output of the non-cat as well then
  the model also gives the picture of non-cat then it is true negatives.
  c. False Negatves:
  If the predicted value is No, we were interseted in Yes and the actual value was also Yes 
  then it is false negatives.
  for example: 
  If we have given the model a picture of cat and we are interested in the output of the cat as well then
  the model gives the picture of non-cat then it is false negatives.
  c. False Positives:
  If the predicted value is Yes, we were interseted in No and the actual value was also No 
  then it is false negatives.
  for example: 
  If we have given the model a picture of non-cat and we are interested in the output of the non-cat as well then
  the model gives the picture of cat then it is false positives.

                                        Actual
                              Positive(1)   Negative(0)
                              ---------------------------
               Positive(1)    |     TP      |     FP    |          
                              |             |           |
    Predicted                 |--------------------------    
                              |      FN     |     TN    |
               Negative(0)    |             |           |  
                              ---------------------------
                
                Now the Accuracy in this case will be:
     Accuracy  = True Positives + True Negatives
                  -------------------------------
                          Total Predictions

1. Confusion Metrics- Precison:
     Precision  =         True Positives
                  -------------------------------
                  True Positives + False Positives
It is the number of correct positive results divided by the number of positive results predicted by the classifiers.
Precision emphasises on minimizing the false positives.
It is interested in the predictions.
Use Case: Predicted disease but actually healtly

1. Confusion Metrics- Recall:
     Recall  =         True Positives
                  -------------------------------
                  True Positives + False Negatives

it is the ratio of predicted positive instances to total actual positive instances.
Recall emphasises on minimizing the false negatives.
It is interested in the predictions.
Use Case: Predicted healthy but actually diseased.
In general the confusion metrics have two diagonals, if the true values diagonal is heravy then the model is 
good and if the false diagonal is heavy then the model is not good.

there are multiple model bulding algorithms like decision tree, random forest , logistic regression bu why 
the grading boosting classifiers 
have always the highest accuracy?
Answer:
Unlike Random Forest (which builds many independent trees and takes their average), Gradient Boosting builds one 
tree at a time, 
and each new tree learns from the mistakes of the previous ones.
It tries to reduce the error of the previous model step-by-step (this is the "gradient" part — it minimizes 
the loss function gradually).
for example : Imagine a student correcting mistakes after every exam. By the end, their performance is more refined.

---------------------------------------------------------------------------------------------------------------------------------------------------
Tensor Flow
We have studied the conventional machine learning uptill now. They cannot process the raw data.This is one of the 
limitation (constraint) of the conventional learning.ML models (regressors, classifiers, random forest ,trees) 
find much difficult to handle the structure data.

Deep Learning:
Neural networks are used instead of the conventional machine learning models (regressors, classifiers).
It is the branch of machine learning used to perform the specific tasks.
                                          Artificial Neurons
                                                  |
                                                  |
                                                  |
                                                  |
                                      Artificial Neural network
Linear models do not work in a certain scope
So, here the clear differnce can be seen clearly as
Machine Learning:

Car as input ------>  Person extracting features ------> Classification ------> Output

Deep Learnig:

Car as input ------> extracting features + Classification ------> Output

Neuron is actually a function to used to perform a specific task.
max() function is used as the RELU function.

TensorFlow:
It is a open-source machine learnig framework developed by Google. It enables building, training, and deploying
machine learning models.
It is used for the machine larning tasks.
It is versatile for other types of ML.
Loss:
Distance between the actual and predicted labels defines the loss.Loss must be less or close to 0.
Converge:
In tensorflow blackground, if we see the distribution boundary seperates the two classes very quickly then the 
process is called the converge.
---------------------------------------------------------------------------------------------------------------------


Hyper-Parameters of neural network
                                        Differnce
Parameters                                                          Hyper-parameter
Parameters are extracted from the                                   Hyper-paramter are extracted from the 
data.                                                               hit and trial method/prctice.
For example: we have a ball to throw into a basket with some particular trajectory. So, we try different 
trajectories again and again to put the ball into the basket. So these trajectories are being used by the 
hit and trial method.

Hyper parameters of the neural network model are as:
Epoochs
Learning rate
activation

1. Gradient Descent: (gradient means slope while the descent means the downwards)
It is the hyperparameter that controls the step size during gradient descent optimization.It determines how 
quickly models adjusts its parameters in the direction that reduces the loss.
Higher learning rate might leads to the faster convergence.but risks the over-shooting aswell while the 
lower rate might lowers the convergence.
For example: you are at the top  of the mountain and you have to come down the mountain then you will come down 
using the steps or you will come down using some step size aswell. This step size is called the learning rate
and if the learning rate (step size ) is suitable you will reach the bottom (valley) within a suitable time.
This is called convergence
and if the learning rate (step size ) is fast then you might not converge and you will again climb up the other 
mountain aswell and this is called the divergence.
  Learning rate:
  learning rate is the subpart of the gradient descent or you can say that the learning rate is being used in 
  the gradient descend.

2. Batch size
Batch is the subset of the dataset used in each iteration of the learning process.
instead of processing the entire dataset we process these small subparts(batches)
Let's dive into deep:
Suppose you have a computer which have some specific memory limit and if that memory limit gets exceeded then 
the memory might get crashed. So, the solution is to use the batches(small memory chunks) which can be processed 
sequentially and the memory might work in easy flow as well.
for example : In academies we attempt the test of the specific book for the 16 times in a compete test session
it means that we are not able to memorize the complete book collectively so we decided to make the small batches 
so that these batches (tests) helps us to memorize the complete book easily. 
So the process of pasing through all the tests and coming to the stage that you have memorized the complete book
is called one epoch. 
Number of Epochs:
number of times the complete dataset is senn by the model during the process of training. Too few epochs might 
result into the underfitting and too many epoochs might results into the overfitting.

Activation Functions:
it determine the function applied to the ouputs of each neuron to introduce the non-linearity.
Example:
ReLU (Rectified Linear Unit)
Softmax


Binary classification   results the two outcomes either it is 0(no) or it is 1 (yes)
For example: Sigmoid results into 0  or 1.
sigmoid is the logistic regression
but if we have the multi classes like we want to identify the multiple shapes then here the sigmoid cannot work 
because it can work only with the binary classification models as the sigmoid returns only (0,1)
So, we will be using the softmax here as it can work on the multi-class framework aswell.


In the neural network:
Input                                   Hidden                                        Output
data provided                           passing on the information                    Decision are taken on the 
                                        from one neuron/activtion                     passed(narrowed down) information
                                        function to another                           and the output is generated.
                              
----------------------------------------------------------------------------------------------------------------------
practice:
ReLU and SIGMOID can work only on the single neuron while the SOFTMAX can work on the multiple neuron
OR
ReLU and SIGMOID can work only with the binary classification while the SOFTMAX can work with the multi-class classification aswell
i want to build a neural network model actually its contains the images of handwritten numbers present in the mnist data involved in the dataset of python 
python framework.
It involves the following steps:
1. Required Packages and data
2. Data Splitting
3. Dataset Display
	(Number like handwritten)
4. Flattening the Images
	(reshapping the Images like from 2-d to 1-d, conversion into the vector)
5. Min-Max Scaling
	(As 255 represents the black and 0 represents the white)
	(Scaling so that the edges remain prominent)
6. Processing Target Variable
	(As the classes  are 10)
	(One- hot encoded)
	(Data to categorical)
7. Setting up the hyper-parameter 
	(their setting-up method comes from the practice not directly from the raw data)
Above all the processes were for the data preprocessing
Now we will be looking at the ardhitecture of the model
Standard Neural Network/Fully Connected Neural Network
8. Building up the FCN Model
	(creating the sequenial model)
	(Adding layers)
	(use the dense as it gives the fully connected neurons)
9. Compilation:
	(Checking the loss)
	(multi-class use the crossentropy while the binary class use the binary entropy)
	(optimizer like the "SGD")---socasted gradient descent socasted means the addition of the extra features into the gradient descent
	(Metric accuracy aswell)
10. Parameters Calcualtion:
for instance:
	(400      *      784)    +   400      =   314000)
	1st		Input	     Bias	  Output
	layer		Data
	neurons
11. Traning of the neural network
	(Time of training)
	(fit function implementatio)
12. Testing the models
	(prediction)
	(accuracy score etc)
13. For example
	(Taking 20 sample images)
	(predicted probabilities for selected)
	(validation examples)
14. Setup a figure to display images
	(Display the predicted as well)
------------------------------------------------------------------------------------------------------------------------------------------------------
Convolutional Neural network (CNN)
Actually the CNN is better then the Standard Neural Network (SNN) because of the computational power use as the SNN involves the fully connected 
neurons with each other for carrying out the specific functionality and CNN involves mainly the performance of the filter to carry out the 
specific functionality and that is why less computaional power is needed.

Let' s dive into deep:
When we seen in general we recognize the things in the patterns and these patterns as a whole forms a complete image

CNN is like the operations of addition, multiplication, division etc. but it involves the other features as well. but it involves the more 
functionalities like we have an image in the computer for recognition in the computer as these images are recognized by the computer using the CNN
feature/kernel helps to recognize the image as they apply the specific size filter to the complete matrix of the image to make a complete recognized
matrix from the unrecognized image.
Filter is the main diffence between the SNN and CNN as SNN involves the fully connected neurons while the CNN involves the filters to recognize the 
image or pattern.
Filters are just like the magnifier as the magnifiers existed previously to perform the specific tasks like sober's or shoore 's magnifiers 
but the filter involved in the CNN involves the 
Input image ----------> filters / feature application ---------------> extraction of feature map(which helps to recognize the images)
back propagation also exist in the above process to identify the correct magnifier and this all process needs very small computational power.

Vertical Edge detection:
Training of kernel again and again for its smooth functionality. (this process is very important)

Pooling:
It is the process of summarizing the information and keeing the information which is very important.It is like the dimensionality reduction
keeping the information which is important.
pooling  = downsizing + information summary

Types of pooling:
1. Max Pooling       (Best dimensionality reduction method)
Takes the maximum value from each patch.
Feture retention: keeps the dominant features intact.
Translation invariance: recognizes features regardless of exact location (means that will recognize the face from the image wherever
it exists in the image )
dimension reduction: reduces data size and improves the efficiency.

2. Average Pooling     (second best)
Computes the average of values in the patch.
Feture retention: Keeps an overall smooth idea but less focus on dominant features.
Translation invariance: Still somewhat translation-invariant.
Dimension reduction: Yes, but might blur sharp features.

3. Sum Pooling    (third best):
Sums all values in the patch.
Feture retention: No emphasis on strong features — adds all regardless of strength.
Translation invariance: Still invariant to small shifts.
Dimension reduction: Yes, but not as effective — larger values may dominate due to addition.
---------------------------------------------------------------------------------------------------------------------------------------------------
                                                                  Difference
ANN(Artificial Neural Network)                                                          CNN(Convolutional Nework Neuron)
ANN reognizes the images block by block.                                                CNN recognizes the images piece by piece
Computationaly Expensive model                                                          Computationaly inexpensive model



Computer takes the images as the matrix.
Neurons in the convolution performs the different operations as compared to the neurons in the SNN/ANN because they perform the other 
functionalities as well as they performs the convolutions as well.
Convolutional Neurons extract features by the pooling then they do the dimensionality reduction, then they send them to the fully 
connected neurons must have the same neurons as the classes exists then softmax function then the result.

CNN Layers involves:
Input             Convolution               Pooling                      FCL(Fully connected layer)                                       Output
Layer             Layer                     Layer                        Classification (binary (sigmoid), multi-class(softmax))             
                                                                         Regression  (relu)

Neuron involves the two parts which are as 
Linear calculation part  (wx+b)
activation function   (relu, sigmoid)
Linear Neurons:
These are the neurons which does not have the activation function.



now i want to do the practical as well as it involves the following steps 
1. Loading the packages
2. Loading the avialable device or connected ones i.e checking cpu or gpu this step is used in the collab
3. loading the dataset
4. Splitting the data 
5. Displaying the data 
6. Displaying the shapes of the data 
7. Reshapping
8. Min Max Scalling 
9. One hot encoding
10. Building the CNN
        (Sequential)
        (Dense,flatten)
        (Conv2d, maxpool)
        (filters i.e (2,32,80,120))
        (classes =10)
11. Creating the model
        (sequentially)
        (now keeping the layers sequentially)
        (like add.conv2d(__) , add.maxpool(__))
        (add.flatten)
        (add.dense)
        (add.compile using the crossentropy, accuracy , sgd)
        (model.summary)
12. parameters calculation:
Layer_1  = filter_height * filter_width * input_channel +1(bias) number of filters = parameters 

13. Training the CNN
14. testing the model
15. taking 20 samples images for checking the model

-----------------------------------------------------------------------------------------------------------------------------------------------------
AutoEncoders
They are also one of the type of neural network model.
Used for the dimensionality reduction and feature learning.
Primary goal is to encode data into the compact represenation and then decode the reconstruction.
Anatomy of Encoders:
Components:
Encoders                                              Bottleneck                                                    Decoders
Compressing much information                          Latent space representation                                   will recreate the from compressed
into less without lossing it                                                                                        to incompressed

Encoder                                                                                       Decoder
Image to vector                                                                               Vector to image 
Data compression                                                                              Data incompression
Feature extraction                                                                            Generates the features using the processed data

It is the responsibility of the decoder to minimize the differences between input and reconstructed input.
to minimize the reconstruction loss.

Encoders converts the original data into the secret code.
uses rules and transformations to hide the message
Decodes reverts the secret code back to the original data.
Understands the rules to reverse the encodings


Types of encoders:
1. Vanilla Autoencoders
2. Denoising Autoencoders (removing the unnecessary parts)
3. Variational Autoencoders


Auto encoders takes us to the non-linearity (which enables us to do complex calculations as well).
-----------------------------------------------------------------------------------------------------------------------------------------------------
Building the Autoencoders
It is one of the type of unsupervised learning as we do not need the input and output labels in this case and we just calculate the loss by 
comparing the input and output labels this is called self-supervised learning one of the type of unsupervised learnings.

Building of Autoencoders involves the following steps
1. Loading the datasets
2. Data Displaying
3. Data Scaling
Encoder Part
4. Architecture of auto-encoder
      Building layers step by step with the specific names as well
                  using activation functions, padding, kernel_initializer
5. Flattening 
Decoder Part
6. Reshaping 
7. conv2d, upsampling
8. Complile 
9. Summary
10. Training the Auto-encoder
11. Prediction
    Passing the encoded images
    


Auto Encoder involves the two parts
1. Encoder
2. Decoder
They can be used single by single as well, means seperately as the seperate models but being in use collectively.




Chatboats Complete Functionality Breakdown:
Human requests some functionality chatboats usually goes to the LLM(Large language model) and brings the outcome from it.
Session:
This all process is being carries out in the session and this session is being created for a specific interval of time or the upper limit 
is set for the sessions. These sessions stores everything into it temporarily and they store it on the client side in the browser that is 
why it is fast but not the secure one as it is more vioable to the hacking attempts as they contains the more risk of hacking that is why the 
sensitive data is not kept into the client side and kept on the server side for the security purposes.
Context Establishment:
It is the most important part while having a chat with the boats as the boats knows the context establishment because they have the strong 
reationship between the generated texts and the prompts given by the user.
How the context establishment is carried out in the chatboats?
Usually when we are communicating with the chatboat they, store the prompts in the client side browser and then send it to the LLM(like chatgpt)
these llms then process the prompts on the server side and gives the response to the chatboats and the complete history is being recorded on the 
both side client side (through the prompts ) as well as on the server side (by the responses generated by the LLMs).

Guardrials:
Actually these are the constraints added to the chatboat for the security purposes.

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Chatgpt Flow:

-------------
| User Query |
-------------
      |
      |
      |
      |
      |
-----------------                           -----------------
| Zipped Message |                          | System Message |
-----------------                           -----------------

                                            -----------------
                                            |  Human Message |
                                            -----------------

                                            --------------                             ------------             ------------                     ----------
                                            |  AI Message |                           | Chat Model |            | Response |                    | Response |
                                            --------------                             ------------             ------------                     ----------
                                                  -
                                                  -
                                                  -
                                            ----------------
                                            | Human Message |
                                            ----------------

                                            --------------
                                            | AI Message  |
                                            --------------


There will be the automatic context establishment and the this context is very important for the complete chat background knowledge.

Size of the zipped message depends upon the LLM as it decides the lenght which is called context window(chat extension capacity).

----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Langchain:
It connects the external data source with the LLM, acts as a bridge between the data source and LLM.
It is a open-source framework designed to simplify the creation of applications using LLM.
Used for:
Chatboats 
Answering questions using sources
data augmentation

Suppose we have a book of 300000 pages and the capacity of LLM is 3000 so the 300000 pages will be divided into the 100 groups of 3000 pages and will be used to maintain the 
contxt window. these divided groups are called the chunks which are inter-related as well.

Document gpt:
Documents                 splits into chunks                        Create embeddings                             Vector Store

Steps for the building of the chatboats:
Splitting data into chunks
Chunks then be converted into the embeddings 
Similarity search on the embeddings 
Answer to the questions using the LLM.

Vector Embeddings:
Numeric conversion of the data to store them for the models so that they can be used by the model.
Human centric language into the machine centric-language(numeric form).
Vector Database/Store:
The place where the data in the form of vectors is stored.

----------------------------------------------------------------------------------------------------------------------------------------------------------------------
                                                              Differnce
Simple/Traditional Database                                                                    Vector Database
Simple database stores the data as it is                                                       Vector database stores the data in the numeric form
means the data is stored in the raw form                                                       Raw form of the data gets converted into the numeric form.
the form in which it exists , gets stored                                                      These numeric forms are actually the embeddings.                                                           
in the same form as well.


Chains:
It is the core building block of the langchain applications.
A chain is a sequence of steps that are performed on the piece of text.
It involves the following steps:
Retrieval           (the retrieval is done through the embeddings)
Generation          (generation is done through the models)
Post-Processing     (post processing is done covering the previous records as well)

                                                              Differnce
Langchain                                                                                            Pipeline
Involvement of LLM                                                                                   Regression/Classification Models
Chain of tasks\functions                                                                             Might be not a model.

Langchain Types:
LLM Chain     (Simplest one)
Retrieval Q/A   (No need to remember the questions and answers)        Retrieval Q/A with reference/source value
Conversation    (There will be the complete of records of questions and answers)


FIASS Vector Store problem:
it stores everything locally and then every i/o request requires the computational power as well thus takes the time in the processing as well.
This problem is solved by the Qdrant which resolves the problem of local storage as well.
Qdrant (Vector Store)
It is a open-source vector database that allows you to store, index and search high dimensional vectors at scale.
Performance and Scalable Solution for Vectors 
Search application                  Question Answering                      Recommendation System
NLP

It gives the following things:
Performance
Scalability
Ease of Use 
Free to Use


---------------------------------------------------------------------------------------------------------------------------------------------------
LlamaIndex:
it is a python framework does exactly the same tasks as the langchain does but it provides us with more ease.
Used for the:
Chatboats making
data connection building using the llms
                                                      Difference
Llamaindex                                                                              Lama-2
It is LLM index that enables the                                                        LLM trained on massive dataset of text and code still 
efficient search and retrieval                                                          under the development already learned to do many tasks
purposes. 
It needs much computational power as the many neurons are                               It needs much computational power as the many neurons are 
working into it.                                                                        working into it as well.  


Uses:
Q/A over the documents
Text Summarization
Code Generation
Creative Thinking 
Data Exploration

                                                      Difference
LamaIndex                                                                                 Langchain
                                                      Purpose
Search and Retrieval purposes.                                                            General processing framework

                                                      Interface
Simple one                                                                                Flexible                                                      

                                                      Efficiency
Very Efficient                                                                            Less efficient

                                                      Use                                        
uses the Application needs to process the large                                           Uses the applications that needs the more 
amount of data.                                                                           customization.

                                                


