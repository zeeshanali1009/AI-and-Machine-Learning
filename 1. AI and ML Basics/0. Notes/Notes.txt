--------------------------------------------------------------------------------------------------------------------
Artificial Intelligence:
"It is a branch of computer science that enables the computer to learn without the explicit programming
(instructions based programming telling the computer to carry out the certain tasks through the instructions)."

Machine Learning:
"Machine learning is the branch of artificial intelligence that makes the computer learn through examples and
experiences. Model gets trained over the new data and this process is kept repeating time to time."

Deep Learning:
"It is the branch of the machine learning that works on the specific functions that sre called neurons as the neurons
are being designed to carry out the specific functionality."

Note:
Every neuron is the function but every function is not the neuron.
Example: RELU Neuron is used to find the maximum number and SIGMOID function is used to convert into the binary from.

There are the two main roles as the 
    a. Encoders (Human centric to machine centric)
        "Human language into the vector embeddings or into the numerical form to make it understandable for the 
         computer."
    b. Decoders (Machine Centric to human centric)
        "Vector embeddings into the human understandable language so that the human can understand it.
This all process is being carried out by the Large Language Model(Text based model or the adversal neural network)."
Diffusion models if there is the dealing with the images
Stable Diffusion:
Role of encoders and the decoders for the image generation as the working is being carried out on the pixel and 
the image is being cleared using this process. Image can also go back to the its original position as this process 
is being used in the security purposes for the theif detection.

Encoders and decoders works collectively to make the Transformers.


Natural Language Processing:
    a. Stemming:
        Remving the last few unnecessary chracters like car from caring to make it a meaningful one.
    b. Lemmatization:
        It removes the last few chracters by understanding the context as well for instance caring to care 
        or eat from ate.
    c. Limmitization:
        It is the more intelligent form as it understands the 1st, 2nd and 3rd form of the verbs as well.
--------------------------------------------------------------------------------------------------------------------
Types of Machine Learning:
----- 1. Supervised Learning    (Labelled Data)
    Types of Supervised Learning:
        a. Classification (Data with classes)
        b. Regression (Score based Answers/Feedbacks)
        Explaination
            a. Classification: 
                "Data is arranged into the classes for instance squares, triangles etc are already being classified
                 as the seperated form."
            b. Regression:
                "Score based answer for instance the output on the essay like asking the model what is your feedback
                 on the essay."


----- 2. Unsupervised Learning  (Unlabelled Data)
"A large record of data later arranged into a group or the cluster for the easy recognition and access.
It is used for the dimensionality reduction as the data with multiple inputs being given as the single output."

----- 3. Reinforcement Learning (Action Based Learning)
"Action based learning which is being carried out on the basis of the feedbacks as there are the three main
chracters in this type of learning which are agent, data , training. Agent is being trained by the use of the 
data through the process of training." For instance the child learns how to ride the bicycle like working as the 
agent in this scenario.
There are the two types of models
a. Training Mode (Model will be getting training)
b. Inference Mode/Prediction Mode  (Testing of the model)

--------------------------------------------------------------------------------------------------------------------
Prompt Engineering:
Prompt engineering deals with the methodology of prompt giving to the computer as it covers various aspects which are
as follow:
-Persona
-Task
-Steps to complete the tasks
-Context
-Constraints
-Goal 
-Format Output

Exploratory Data Analysis (EDA):
It is the process of exploring the data and perfoming the analysis over it.
These are the steps involved into this process
1. Data Analysis
        Data Engineers (Classifies the data)
        Data Analysis  (Gives analysis that what kind of model will be made through the certain type of data)
        Data Scientists (Does everything related to the data)

2. Prprocess the data (Understands the data):
a. Data Cleaning:
       Removing the unnecessary stuff from the data
b. Removing the duplication:
       Removing the duplication from the data as ther will be more light weight data
c. Data Transformation:
       Transforming the data from one certain form to another certain form.
--------------------------------------------------------------------------------------------------------------------
Follwinng are the steps involved into the EDA:
-Data Cleaning
-Descriptive Statistics (Summary Statistics)
            Mean
            Median
            Mode (Multi model)
            Inter Quartile Range (Q3-Q1)
            Standard deviation
            Dispersion
                Less dispersion means less training of the model so the less intelligent model will become and 
                greater dispersion means there will be the more training of the model and more intelligent will be 
                the model.
                Example
                                12         34       56        78        12      34        67         89         90       23 
                                sorting
                                12         12       23        34        34      56        67         78         89       90
                                min                                     34+56/2= 45                                      max
                                                                          Q2 = 45
                                           12+23/2=17.5                                               78+89/2=83.5
                                           Q1 = 18                                                    Q3 = 84

                                IQR = Q3 - Q1
                                    = 84 - 18
                                    = 66

-Data Visualization
            Data will be made visualizable
-Data Distribution
-Correlation Analysis
            Relationships of the features of the dataframe among each other and how they are depended on each other.
            It can be positive (strong) or it can be negative (weak) relationships as well.
-Outlier detection
            Outlier from the given range is detected and gets ignored as the outlier changes the direction in the 
            Average getting process.
-Data Transformation:
            Data is transformed from one place into the other one.

A coloumn can have different names in the form of:
Feature  (ML)
Attribute  (Database)
Variable (Statistics)
--------------------------------------------------------------------------------------------------------------------

Classification:
Classification is a machine learning tecnology used to predict the categorical data or discrete target variables.
It is one of the type of supervised learning which is carries out usiing the input/output pairs as the complete 
process is supervised.Models are also trained using the Input/Output pairs.
In classification the response will always be nominal/categorical/label data.
In classification, the classification name is classification label.
Types of Classification:
1. Binary Classification
    There can only be two states at one time either it is yes(1) or it is no(0).Now here the values are numeric but 
    we know the numeric answer/output is possible only in the regression and here 0,1 are numeric although it is in
    classification. So these numeric numbers are just for the representation, numeric operations cannot be performed
    on them.
2. Multi-class Classification
    More than two classes at one time.
Target Variables:
It is the variable we want to predict.
It represents the outcome or label we are interested in.
EDA helps us to find the target variable.
--------------------------------------------------------------------------------------------------------------------
There are two types of the variables which are as follow:
Dependent                               Independent
values depends upon                     Values does not depends upon 
other features                          other features.
EDA is done in the data engineering and it is done before providing the data to the ML engineer for the
machine learning purposes like the model training.
Regression:
It is a statistical technique used to model the relationship between a dependant variable and one or more
independent variables.
Dependent variables are said as "outcome" or "target".
Independent variables are said as "variables" or "features".
                                                Differences
Regression                                                         Correlation
-Predicts the values of dependant                |       -Access the strength and direction of the
variable based on independent                    |       linear relationship between variables.
variables.                                       |
-Predictive and Exploratory Modelling            |       -Descriptive analysis
-Provides the values which represents            |       -Gives a single-value (corelation-efficient)
the effect.                                      |        representing the strenght of corelation.
--------------------------------------------------------------------------------------------------------------------
Logistic Regression(classification):
It is a type of analysis for predicting the binary outcomes (two categories) 
For example: predicting whether a student will pass (1) or the student will fail(0).
having some sort of probability.
Graph:
                              |
                            1 |__________________******_______
      Dependent               |
        Varibles              |
                          0.5 |---------------*-------------   Threshold        Actually there is a s type line on the
                              |                                                 graph too
                              |
                          0   |__*******_______________________
                                  Independent variables       x

It gives two numbers (0,1) in binary.Regression gives the value which gives the insights.
--------------------------------------------------------------------------------------------------------------------

Linear Regression:
It is widely used in various fields , including finance, economics, and data sciences.Great for making the predictions 
and understanding the relationship between variables.
Score whcich is based on the lines.
Graph:
                              |             *-
                            1 |     -  -   *-
      Dependent               |    -  ---*-
        Varibles              |    -  -*-
                          0.5 |    -*- -                                       Actually there is a s slope line on the
                              |   *  -----                                          graph having multiples around it.
                              | *
                          0   |*_________________________
                                  Independent variables       x
We have to choose the line which must be close to the datapoints.Linear regression model chooses that line as well.
Line/Actual line/Ideal Line  is like  y =mx +c  (Slope intercept form)
Y: Dependant variable (Output)
x: InDependant variable (Input/feature)
m: Slope of line (how much y changes when x is 0)
c: the y intercept (the value of y when x is 0)

Loss/Error:
How far the line is from the datapoints
Actually when the error is in negative we have to extract the positive out of it to take the positive out of errors.
Mean Squared Error(MSE):
It ia common evaluation metric used in the regression task. It measures the average squared difference between the 
actual line and predicted line.
As MSE increases the Line Increases
As MSE decreases the Line decreases
Mean Absolute Error:
Taking the absolute value from the complete value 
for example : 40 from -40

Dimensionality Reduction:
Data trimming/ Removing the irrelevant information.
If dataset have 12 cols it means that the dataset have 12 dimensions
--------------------------------------------------------------------------------------------------------------------
Scikit-learn:
It is the popular python library used for the classification, regression and clustering aswell.

Data Splitting
Data is splitted into two different categories which are as:
Training Data
Testing Data

--------------------------------------------------------------------------------------------------------------------
Evaluation metrics for different models:
If the model is giving too much big MSE error, what could be the problems?
Data preprocessing falults (might be)
feature engineering required  (might be)
One thing is sure, error will stay forever, the problem is how to mitigate (minimize) this error.
MSE is directly porportional to the models performance.
A plane has the 2d or 3d graph (graph between the dependent and independent variables)
(most of the times the dependent variable is one while the independent variables are more then one.)
Data Preprocessing:
Data reduction
Data Transformation
Now, this is the main problem for the models to give the mse value more like there exists many features whose values 
are in different ranges which are need to be in the same range for the better performance of the model.We have
some standards to bring them in the same range which are as follow:
MinMax Scaler
ZeroScale/Standard Scaler
Standardization:
it is the type of data preprocessing that makes the different features have a similar scale. like in (0,1 range)

Differnt Regression Techniques:
1. Decision Tree Regression:
It involves the partitioning of the data into subsets based on the values of the independent variables and predict
the average of the target variable.
for example you have 100 different records of the players and you have given these records to the 5 categories
of differnt people (0-25), (26,50), (51,75), (76,100). Now you can get the score of the individual player easily
by calling the group which have the record of that particular player.
Like it is in this pattern:
                                         0
                                       *  *
                                      *     *
                                    *         *
                                    0          0
                                  *   *       *  *
                                *       *    *    *
                                0        0   0      0

2. Random forest regressor:
it is the learning method that creates the multiple decision trees during training and outputs the average 
prediction (for regression) from all individual trees
for example: Suppose you are in a sitting of 10 differt people and you have to make the decision on one particular
thing then you have 10 differnt people for the advice purposes and there advice also acts as the vote
in the form of pattern it would be like:
                                        x
                                       *  *
                                      *     *
                                    *         *
                                  Tree       Tree   upto tree(n)
                                    *          *            *
                                     *        *            *
                                       *     *            *
                                         *  *           *
                                           +
                                           y(outcome/result)

3. Gradient Boosting Regressors:
it is the extension of decision trees.
gradient boosting is learning techniques hat builds multiple decision trees sequentially each one correcting
the error of predecessors
for example: you have a sitting of 10 people and every person gives you an advice to move on and you act upon the 
advises and if you make a fault after following up the advise then once again you take up the advise and act upon it
and this process goes on.
-------------------------------------------------------------------------------------------------------------------
Further Explaination of Evaluation Metrics:
Entropy:
Measure of impurity.
for example: we have the three containers which have the eggs and candies in them like:
Container 01                       Container 02                     Container 03
| eccececec |                     | eeeeeeeee |                     | eeeeeeeee |
| ecececcec |                     | ccccccccc |                     | eeeeeeeee |
| ecececeec |                     | eeeeeeeee |                     | eeeeeeeee |
|-----------|                     |-----------|                     |-----------|
More Impurity                     Less Impurity                      No Impurity
More Entropy                      Less Entropy                       No Entropy
Entropy is used to find the target variable, as we want that feature as the target variable whose entropy is less
means that it must have less impurity into it. 
In the decision tree, there is the decision going on every node, in fact the decision is represented by the node
On the leaf, there is the only single class.
Decision tree is the computation friendly.
It can also work with the categorical data although other algorithms mostly likes to work with the other form of
data (mostly numeric).

Decision Tree Classifiers:
it makes the decision by splitting the data into a smaller subsets based on the values of input features
ultimately assigned a class label to each data point.It constructs a tree like data structure of decision rules 
that can be used for prediction and is easy to interpret.

Random Forest Classifier:
It is the ensemble learning method that creates the multiple decision trees during training and outputs the 
average prediction from all the individual trees.
It will have different root nodes as different target variables(can be multiple as the multiple trees are 
in the forest with different feature as the target variable)

Ensemble: 
Ensemble learning is a machine learning technique that combines the predictions generalizations. Instead of relying 
on a single model ensemble methods create a "commitee" of models and aggregate their predictions to make a final 
decision.

Min/Max Scaling:
It is a data normalization technique used to transform the data into a specific range.
formula :       x' = x- min(x)/ max(x) - min(x)   (1-0)
it saves the computational power as well.
For example:        Normalize the data [200,300,400,600,1000]
                    min(x)  = 200           max(x)  = 1000
                    scaled(x)  = (200-200)/(100-200)* (1-0)
                    = 0
                    same like this the complete form would be like
                    [0, 0.125, 0.25, 0.5, 1]

Z-Score Scaling:
It is also knowns as the standardization, is a technique used to transform the data into a standard normal 
Distribution.
formula :             Z=   score  - mean / standard deviation
For example:        Normalize the data [200,300,400,600,1000]
                    min(x)  = 200           max(x)  = 1000
                    mean  = 50
                    standard deviation = 286.4789
                    now the transformed dataset will be like
                    [-1.047, 0.698, -0,349, 0.349, 1.747]
Now we have an exampele as 
Suppose a dataset like: (have 4 features)
Business              Competetion             Values                Profit
Freelancing           Low                     Low                   Yes
E-Commerce            Medium                  Medium                No
E-Commerce            High                    High                  No
Software house        Low                     Medium                Yes
Freelancing           High                    High                  yes
Software house        Low                     Medium                no

Now we have to make the decision tree from it as well like


                                














