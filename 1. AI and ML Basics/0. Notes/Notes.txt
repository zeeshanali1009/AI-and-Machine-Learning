--------------------------------------------------------------------------------------------------------------------
Artificial Intelligence:
"It is a branch of computer science that enables the computer to learn without the explicit programming
(instructions based programming telling the computer to carry out the certain tasks through the instructions)."

Machine Learning:
"Machine learning is the branch of artificial intelligence that makes the computer learn through examples and
experiences. Model gets trained over the new data and this process is kept repeating time to time."

Deep Learning:
"It is the branch of the machine learning that works on the specific functions that sre called neurons as the neurons
are being designed to carry out the specific functionality."

Note:
Every neuron is the function but every function is not the neuron.
Example: RELU Neuron is used to find the maximum number and SIGMOID function is used to convert into the binary from.

There are the two main roles as the 
    a. Encoders (Human centric to machine centric)
        "Human language into the vector embeddings or into the numerical form to make it understandable for the 
         computer."
    b. Decoders (Machine Centric to human centric)
        "Vector embeddings into the human understandable language so that the human can understand it.
This all process is being carried out by the Large Language Model(Text based model or the adversal neural network)."
Diffusion models if there is the dealing with the images
Stable Diffusion:
Role of encoders and the decoders for the image generation as the working is being carried out on the pixel and 
the image is being cleared using this process. Image can also go back to the its original position as this process 
is being used in the security purposes for the theif detection.

Encoders and decoders works collectively to make the Transformers.


Natural Language Processing:
    a. Stemming:
        Remving the last few unnecessary chracters like car from caring to make it a meaningful one.
    b. Lemmatization:
        It removes the last few chracters by understanding the context as well for instance caring to care 
        or eat from ate.
    c. Limmitization:
        It is the more intelligent form as it understands the 1st, 2nd and 3rd form of the verbs as well.
--------------------------------------------------------------------------------------------------------------------
Types of Machine Learning:
----- 1. Supervised Learning    (Labelled Data)
    Types of Supervised Learning:
        a. Classification (Data with classes)
        b. Regression (Score based Answers/Feedbacks)
        Explaination
            a. Classification: 
                "Data is arranged into the classes for instance squares, triangles etc are already being classified
                 as the seperated form."
            b. Regression:
                "Score based answer for instance the output on the essay like asking the model what is your feedback
                 on the essay."


----- 2. Unsupervised Learning  (Unlabelled Data)
"A large record of data later arranged into a group or the cluster for the easy recognition and access.
It is used for the dimensionality reduction as the data with multiple inputs being given as the single output."

----- 3. Reinforcement Learning (Action Based Learning)
"Action based learning which is being carried out on the basis of the feedbacks as there are the three main
chracters in this type of learning which are agent, data , training. Agent is being trained by the use of the 
data through the process of training." For instance the child learns how to ride the bicycle like working as the 
agent in this scenario.
There are the two types of models
a. Training Mode (Model will be getting training)
b. Inference Mode/Prediction Mode  (Testing of the model)

--------------------------------------------------------------------------------------------------------------------
Prompt Engineering:
Prompt engineering deals with the methodology of prompt giving to the computer as it covers various aspects which are
as follow:
-Persona
-Task
-Steps to complete the tasks
-Context
-Constraints
-Goal 
-Format Output

Exploratory Data Analysis (EDA):
It is the process of exploring the data and perfoming the analysis over it.
These are the steps involved into this process
1. Data Analysis
        Data Engineers (Classifies the data)
        Data Analysis  (Gives analysis that what kind of model will be made through the certain type of data)
        Data Scientists (Does everything related to the data)

2. Prprocess the data (Understands the data):
a. Data Cleaning:
       Removing the unnecessary stuff from the data
b. Removing the duplication:
       Removing the duplication from the data as ther will be more light weight data
c. Data Transformation:
       Transforming the data from one certain form to another certain form.
--------------------------------------------------------------------------------------------------------------------
Follwinng are the steps involved into the EDA:
-Data Cleaning
-Descriptive Statistics (Summary Statistics)
            Mean
            Median
            Mode (Multi model)
            Inter Quartile Range (Q3-Q1)
            Standard deviation
            Dispersion
                Less dispersion means less training of the model so the less intelligent model will become and 
                greater dispersion means there will be the more training of the model and more intelligent will be 
                the model.
                Example
                                12         34       56        78        12      34        67         89         90       23 
                                sorting
                                12         12       23        34        34      56        67         78         89       90
                                min                                     34+56/2= 45                                      max
                                                                          Q2 = 45
                                           12+23/2=17.5                                               78+89/2=83.5
                                           Q1 = 18                                                    Q3 = 84

                                IQR = Q3 - Q1
                                    = 84 - 18
                                    = 66

-Data Visualization
            Data will be made visualizable
-Data Distribution
-Correlation Analysis
            Relationships of the features of the dataframe among each other and how they are depended on each other.
            It can be positive (strong) or it can be negative (weak) relationships as well.
-Outlier detection
            Outlier from the given range is detected and gets ignored as the outlier changes the direction in the 
            Average getting process.
-Data Transformation:
            Data is transformed from one place into the other one.

A coloumn can have different names in the form of:
Feature  (ML)
Attribute  (Database)
Variable (Statistics)
--------------------------------------------------------------------------------------------------------------------

Classification:
Classification is a machine learning tecnology used to predict the categorical data or discrete target variables.
It is one of the type of supervised learning which is carries out usiing the input/output pairs as the complete 
process is supervised.Models are also trained using the Input/Output pairs.
In classification the response will always be nominal/categorical/label data.
In classification, the classification name is classification label.
Types of Classification:
1. Binary Classification
    There can only be two states at one time either it is yes(1) or it is no(0).Now here the values are numeric but 
    we know the numeric answer/output is possible only in the regression and here 0,1 are numeric although it is in
    classification. So these numeric numbers are just for the representation, numeric operations cannot be performed
    on them.
2. Multi-class Classification
    More than two classes at one time.
Target Variables:
It is the variable we want to predict.
It represents the outcome or label we are interested in.
EDA helps us to find the target variable.
--------------------------------------------------------------------------------------------------------------------
There are two types of the variables which are as follow:
Dependent                               Independent
values depends upon                     Values does not depends upon 
other features                          other features.
EDA is done in the data engineering and it is done before providing the data to the ML engineer for the
machine learning purposes like the model training.
Regression:
It is a statistical technique used to model the relationship between a dependant variable and one or more
independent variables.
Dependent variables are said as "outcome" or "target".
Independent variables are said as "variables" or "features".
                                                Differences
Regression                                                         Correlation
-Predicts the values of dependant                |       -Access the strength and direction of the
variable based on independent                    |       linear relationship between variables.
variables.                                       |
-Predictive and Exploratory Modelling            |       -Descriptive analysis
-Provides the values which represents            |       -Gives a single-value (corelation-efficient)
the effect.                                      |        representing the strenght of corelation.
--------------------------------------------------------------------------------------------------------------------
Logistic Regression(classification):
It is a type of analysis for predicting the binary outcomes (two categories) 
For example: predicting whether a student will pass (1) or the student will fail(0).
having some sort of probability.
Graph:
                              |
                            1 |__________________******_______
      Dependent               |
        Varibles              |
                          0.5 |---------------*-------------   Threshold        Actually there is a s type line on the
                              |                                                 graph too
                              |
                          0   |__*******_______________________
                                  Independent variables       x

It gives two numbers (0,1) in binary.Regression gives the value which gives the insights.
--------------------------------------------------------------------------------------------------------------------

Linear Regression:
It is widely used in various fields , including finance, economics, and data sciences.Great for making the predictions 
and understanding the relationship between variables.
Score whcich is based on the lines.
Graph:
                              |             *-
                            1 |     -  -   *-
      Dependent               |    -  ---*-
        Varibles              |    -  -*-
                          0.5 |    -*- -                                       Actually there is a s slope line on the
                              |   *  -----                                          graph having multiples around it.
                              | *
                          0   |*_________________________
                                  Independent variables       x
We have to choose the line which must be close to the datapoints.Linear regression model chooses that line as well.
Line/Actual line/Ideal Line  is like  y =mx +c  (Slope intercept form)
Y: Dependant variable (Output)
x: InDependant variable (Input/feature)
m: Slope of line (how much y changes when x is 0)
c: the y intercept (the value of y when x is 0)

Loss/Error:
How far the line is from the datapoints
Actually when the error is in negative we have to extract the positive out of it to take the positive out of errors.
Mean Squared Error(MSE):
It ia common evaluation metric used in the regression task. It measures the average squared difference between the 
actual line and predicted line.
As MSE increases the Line Increases
As MSE decreases the Line decreases
Mean Absolute Error:
Taking the absolute value from the complete value 
for example : 40 from -40

Dimensionality Reduction:
Data trimming/ Removing the irrelevant information.
If dataset have 12 cols it means that the dataset have 12 dimensions
--------------------------------------------------------------------------------------------------------------------
Scikit-learn:
It is the popular python library used for the classification, regression and clustering aswell.

Data Splitting
Data is splitted into two different categories which are as:
Training Data
Testing Data

--------------------------------------------------------------------------------------------------------------------
Evaluation metrics for different models:
If the model is giving too much big MSE error, what could be the problems?
Data preprocessing falults (might be)
feature engineering required  (might be)
One thing is sure, error will stay forever, the problem is how to mitigate (minimize) this error.
MSE is directly porportional to the models performance.
A plane has the 2d or 3d graph (graph between the dependent and independent variables)
(most of the times the dependent variable is one while the independent variables are more then one.)
Data Preprocessing:
Data reduction
Data Transformation
Now, this is the main problem for the models to give the mse value more like there exists many features whose values 
are in different ranges which are need to be in the same range for the better performance of the model.We have
some standards to bring them in the same range which are as follow:
MinMax Scaler
ZeroScale/Standard Scaler
Standardization:
it is the type of data preprocessing that makes the different features have a similar scale. like in (0,1 range)

Differnt Regression Techniques:
1. Decision Tree Regression:
It involves the partitioning of the data into subsets based on the values of the independent variables and predict
the average of the target variable.
for example you have 100 different records of the players and you have given these records to the 5 categories
of differnt people (0-25), (26,50), (51,75), (76,100). Now you can get the score of the individual player easily
by calling the group which have the record of that particular player.
Like it is in this pattern:
                                         0
                                       *  *
                                      *     *
                                    *         *
                                    0          0
                                  *   *       *  *
                                *       *    *    *
                                0        0   0      0

2. Random forest regressor:
it is the learning method that creates the multiple decision trees during training and outputs the average 
prediction (for regression) from all individual trees
for example: Suppose you are in a sitting of 10 differt people and you have to make the decision on one particular
thing then you have 10 differnt people for the advice purposes and there advice also acts as the vote
in the form of pattern it would be like:
                                        x
                                       *  *
                                      *     *
                                    *         *
                                  Tree       Tree   upto tree(n)
                                    *          *            *
                                     *        *            *
                                       *     *            *
                                         *  *           *
                                           +
                                           y(outcome/result)

3. Gradient Boosting Regressors:
it is the extension of decision trees.
gradient boosting is learning techniques hat builds multiple decision trees sequentially each one correcting
the error of predecessors
for example: you have a sitting of 10 people and every person gives you an advice to move on and you act upon the 
advises and if you make a fault after following up the advise then once again you take up the advise and act upon it
and this process goes on.
-------------------------------------------------------------------------------------------------------------------
Further Explaination of Evaluation Metrics:
Entropy:
Measure of impurity.
for example: we have the three containers which have the eggs and candies in them like:
Container 01                       Container 02                     Container 03
| eccececec |                     | eeeeeeeee |                     | eeeeeeeee |
| ecececcec |                     | ccccccccc |                     | eeeeeeeee |
| ecececeec |                     | eeeeeeeee |                     | eeeeeeeee |
|-----------|                     |-----------|                     |-----------|
More Impurity                     Less Impurity                      No Impurity
More Entropy                      Less Entropy                       No Entropy
Entropy is used to find the target variable, as we want that feature as the target variable whose entropy is less
means that it must have less impurity into it. 
In the decision tree, there is the decision going on every node, in fact the decision is represented by the node
On the leaf, there is the only single class.
Decision tree is the computation friendly.
It can also work with the categorical data although other algorithms mostly likes to work with the other form of
data (mostly numeric).

Decision Tree Classifiers:
it makes the decision by splitting the data into a smaller subsets based on the values of input features
ultimately assigned a class label to each data point.It constructs a tree like data structure of decision rules 
that can be used for prediction and is easy to interpret.

Random Forest Classifier:
It is the ensemble learning method that creates the multiple decision trees during training and outputs the 
average prediction from all the individual trees.
It will have different root nodes as different target variables(can be multiple as the multiple trees are 
in the forest with different feature as the target variable)

Ensemble: 
Ensemble learning is a machine learning technique that combines the predictions generalizations. Instead of relying 
on a single model ensemble methods create a "commitee" of models and aggregate their predictions to make a final 
decision.

Min/Max Scaling:
It is a data normalization technique used to transform the data into a specific range.
formula :       x' = x- min(x)/ max(x) - min(x)   (1-0)
it saves the computational power as well.
For example:        Normalize the data [200,300,400,600,1000]
                    min(x)  = 200           max(x)  = 1000
                    scaled(x)  = (200-200)/(100-200)* (1-0)
                    = 0
                    same like this the complete form would be like
                    [0, 0.125, 0.25, 0.5, 1]

Z-Score Scaling:
It is also knowns as the standardization, is a technique used to transform the data into a standard normal 
Distribution.
formula :             Z=   score  - mean / standard deviation
For example:        Normalize the data [200,300,400,600,1000]
                    min(x)  = 200           max(x)  = 1000
                    mean  = 50
                    standard deviation = 286.4789
                    now the transformed dataset will be like
                    [-1.047, 0.698, -0,349, 0.349, 1.747]
Now we have an exampele as 
Suppose a dataset like: (have 4 features)
Business              Competetion             Values                Profit
Freelancing           Low                     Low                   Yes
E-Commerce            Medium                  Medium                No
E-Commerce            High                    High                  No
Software house        Low                     Medium                Yes
Freelancing           High                    High                  yes
Software house        Low                     Medium                no

Now we have to make the decision tree from it as well like
--------------------------------------------------------------------------------------------------------------------
Evaluation Metrics for the models:
This all the following evaluation will be only done on the binary classification models only.
There exists different evaluation metric for the models to verify whether they are performing or not.
1. Accuracy:
It is applied only on the classification, not applicable on the regression.
It is the ratio of number of correct predictions to the total number of input predictions.
formula:                  Accuracy  = Number of correct predictions
                                      -----------------------------
                                      Total number of predictions
It works only if there are equal number of samples belonging to each class.

Class Imbalance:
It is the imbalance of the data between two classes as:

                |   -----   
                |   |   |
                |   |   |
                |   |   |   
                |   |   |   ----
                |   |   |   |  |
                ---------------------------
                    90%     10%

                                              Differnce
            Imbalance                          |                         Skewness
            it is only found in the            |                      It is found only in the numerical
            categorical data                   |                       data.

Data Balance:
If the classes have the balance like 50% - 50% 
or 60% - 40% is also acceptable.
Accuracy does not always gives the good model check it sometimes confuses us by giving the higher values but 
actualy the model is not trained good. so we have to use some other metrics as well to check the model.
Evaluation Metrics:
1. Confusion Metrics:
it gives us a metrix as the output and describes the complete performance of the model.
These are the following terminologies:
  a. True Positives:
  If the predicted value is Yes, we were also interseted in Yes and the actual value was also Yes 
  then it is true positive.
  for example: 
  If we have given the model a picture of cat and we are interested in the output of the cat as well then
  the model also gives the picture of cat then it is true positives.
  b. True Negatves:
  If the predicted value is No, we were also interseted in No and the actual value was also No 
  then it is true negatives.
  for example: 
  If we have given the model a picture of non-cat and we are interested in the output of the non-cat as well then
  the model also gives the picture of non-cat then it is true negatives.
  c. False Negatves:
  If the predicted value is No, we were interseted in Yes and the actual value was also Yes 
  then it is false negatives.
  for example: 
  If we have given the model a picture of cat and we are interested in the output of the cat as well then
  the model gives the picture of non-cat then it is false negatives.
  c. False Positives:
  If the predicted value is Yes, we were interseted in No and the actual value was also No 
  then it is false negatives.
  for example: 
  If we have given the model a picture of non-cat and we are interested in the output of the non-cat as well then
  the model gives the picture of cat then it is false positives.

                                        Actual
                              Positive(1)   Negative(0)
                              ---------------------------
               Positive(1)    |     TP      |     FP    |          
                              |             |           |
    Predicted                 |--------------------------    
                              |      FN     |     TN    |
               Negative(0)    |             |           |  
                              ---------------------------
                
                Now the Accuracy in this case will be:
     Accuracy  = True Positives + True Negatives
                  -------------------------------
                          Total Predictions

1. Confusion Metrics- Precison:
     Precision  =         True Positives
                  -------------------------------
                  True Positives + False Positives
It is the number of correct positive results divided by the number of positive results predicted by the classifiers.
Precision emphasises on minimizing the false positives.
It is interested in the predictions.
Use Case: Predicted disease but actually healtly

1. Confusion Metrics- Recall:
     Recall  =         True Positives
                  -------------------------------
                  True Positives + False Negatives

it is the ratio of predicted positive instances to total actual positive instances.
Recall emphasises on minimizing the false negatives.
It is interested in the predictions.
Use Case: Predicted healthy but actually diseased.
In general the confusion metrics have two diagonals, if the true values diagonal is heravy then the model is 
good and if the false diagonal is heavy then the model is not good.

there are multiple model bulding algorithms like decision tree, random forest , logistic regression bu why 
the grading boosting classifiers 
have always the highest accuracy?
Answer:
Unlike Random Forest (which builds many independent trees and takes their average), Gradient Boosting builds one 
tree at a time, 
and each new tree learns from the mistakes of the previous ones.
It tries to reduce the error of the previous model step-by-step (this is the "gradient" part — it minimizes 
the loss function gradually).
for example : Imagine a student correcting mistakes after every exam. By the end, their performance is more refined.

---------------------------------------------------------------------------------------------------------------------------------------------------
Tensor Flow
We have studied the conventional machine learning uptill now. They cannot process the raw data.This is one of the 
limitation (constraint) of the conventional learning.ML models (regressors, classifiers, random forest ,trees) 
find much difficult to handle the structure data.

Deep Learning:
Neural networks are used instead of the conventional machine learning models (regressors, classifiers).
It is the branch of machine learning used to perform the specific tasks.
                                          Artificial Neurons
                                                  |
                                                  |
                                                  |
                                                  |
                                      Artificial Neural network
Linear models do not work in a certain scope
So, here the clear differnce can be seen clearly as
Machine Learning:

Car as input ------>  Person extracting features ------> Classification ------> Output

Deep Learnig:

Car as input ------> extracting features + Classification ------> Output

Neuron is actually a function to used to perform a specific task.
max() function is used as the RELU function.

TensorFlow:
It is a open-source machine learnig framework developed by Google. It enables building, training, and deploying
machine learning models.
It is used for the machine larning tasks.
It is versatile for other types of ML.
Loss:
Distance between the actual and predicted labels defines the loss.Loss must be less or close to 0.
Converge:
In tensorflow blackground, if we see the distribution boundary seperates the two classes very quickly then the 
process is called the converge.
---------------------------------------------------------------------------------------------------------------------


Hyper-Parameters of neural network
                                        Differnce
Parameters                                                          Hyper-parameter
Parameters are extracted from the                                   Hyper-paramter are extracted from the 
data.                                                               hit and trial method/prctice.
For example: we have a ball to throw into a basket with some particular trajectory. So, we try different 
trajectories again and again to put the ball into the basket. So these trajectories are being used by the 
hit and trial method.

Hyper parameters of the neural network model are as:
Epoochs
Learning rate
activation

1. Gradient Descent: (gradient means slope while the descent means the downwards)
It is the hyperparameter that controls the step size during gradient descent optimization.It determines how 
quickly models adjusts its parameters in the direction that reduces the loss.
Higher learning rate might leads to the faster convergence.but risks the over-shooting aswell while the 
lower rate might lowers the convergence.
For example: you are at the top  of the mountain and you have to come down the mountain then you will come down 
using the steps or you will come down using some step size aswell. This step size is called the learning rate
and if the learning rate (step size ) is suitable you will reach the bottom (valley) within a suitable time.
This is called convergence
and if the learning rate (step size ) is fast then you might not converge and you will again climb up the other 
mountain aswell and this is called the divergence.
  Learning rate:
  learning rate is the subpart of the gradient descent or you can say that the learning rate is being used in 
  the gradient descend.

2. Batch size
Batch is the subset of the dataset used in each iteration of the learning process.
instead of processing the entire dataset we process these small subparts(batches)
Let's dive into deep:
Suppose you have a computer which have some specific memory limit and if that memory limit gets exceeded then 
the memory might get crashed. So, the solution is to use the batches(small memory chunks) which can be processed 
sequentially and the memory might work in easy flow as well.
for example : In academies we attempt the test of the specific book for the 16 times in a compete test session
it means that we are not able to memorize the complete book collectively so we decided to make the small batches 
so that these batches (tests) helps us to memorize the complete book easily. 
So the process of pasing through all the tests and coming to the stage that you have memorized the complete book
is called one epoch. 
Number of Epochs:
number of times the complete dataset is senn by the model during the process of training. Too few epochs might 
result into the underfitting and too many epoochs might results into the overfitting.

Activation Functions:
it determine the function applied to the ouputs of each neuron to introduce the non-linearity.
Example:
ReLU (Rectified Linear Unit)
Softmax


Binary classification   results the two outcomes either it is 0(no) or it is 1 (yes)
For example: Sigmoid results into 0  or 1.
sigmoid is the logistic regression
but if we have the multi classes like we want to identify the multiple shapes then here the sigmoid cannot work 
because it can work only with the binary classification models as the sigmoid returns only (0,1)
So, we will be using the softmax here as it can work on the multi-class framework aswell.


In the neural network:
Input                                   Hidden                                        Output
data provided                           passing on the information                    Decision are taken on the 
                                        from one neuron/activtion                     passed(narrowed down) information
                                        function to another                           and the output is generated.
                                







