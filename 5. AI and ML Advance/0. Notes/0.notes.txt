Artifical Intelligence:
    1. ANI (Artificial Narrow Intelligence)
        like smart speakers, self driving cars, web search, AI in farming and factories.
    2. AGI (Artificial General Intelligence)
        do anything that human can do.

 Artifical Intelligence
 It is the branch of computer science concerned with development of methods that allows computer to learn without explicit programming.
 Search algorithms, rule based system, ststistical inference, machine learning.
    Machine Learning
    It is the branch of artificial intelligence that can learn from examples and experience instead of relying on hard-coded rules and make predictions on new data.
    svm, tree algorithms, nearest neighbors, bagging, boosting, deep learning.
        Deep Learning
        It is the subfield of machine learning that focuses on neural networks (inspired from biological neurons) to deveelop learning models.
        fcns, cnns, rnns, transformers, autoencoders, gans
------------------------------------------------------------------------------------------------------------------------------------------------------------
Samples and datapoints are the same terminologies.
Neuron:
Neuron is the most fundamntal part of the deep learning which contains the three basic things: input interconnections and the output.
Neuron is basically a function that performs a specific function.
Neurons need great:
data
computational power
well trained algorithms

Artificial intelligence is distributed into two parts
1. Descriminative (Creates the patterns, descriminate the data, make the seperations, involes the conventional models that deals with structured data better)
    1. Supervised Learning
    2. Unsupervised learning
    3. Reinforcement learning
2. Generative (generates the information can deal with both structured and unstructured data as well)
    1. NLP(Natural Language Processing)
        It deals with the natural language with the text and with the neurons that deals with this sort of data.
    2. Speech Recognition
        It deals with the speech,voice related data.
    3. Computer Vision
        it deals with the images, videoes.

Generative AI Models:
1. LLM
    LLMs are the generative pretrained transformers that are too large such as chatgpt 3 had 176 billion parameters/neurons.
    Tokenization is the most important concept in the NLP which is the process of breaking down the tecxt into the smaller units called the tokens. which 
    can be words, phrases even the chracters. like it is the vocabulary of the models. token can be single chracter, word or the sentence.
    like: This is the first step in NLP has the tokens t h i s i s t h e f i r s t .........
    context window: it is the limit of the model to accept , process, the tokens like gpt has the 4k and so on etc.
2. Latent Diffusion Models
    Diffusion means going from high intensity towards the lower intensity (In generic) like the ink drop being fallen on the shirt and then starts to spread slowly.
    Stable diffusion
    Latent diffusion model (latent means hidden)
    Actually when we say that the picture is 1024 x 1024 it means that there are many pixels in it (like in millions) one individual pixel has three colors rgb so there is too much 
    computation required for the images in the ai field then for the minimization of the computation we use the latent vectors behind that reduces the computation power to 48 times.
    hence the model is called the diffusion model.
   
    Actually the noise is being added into the picture unite time after unit time and then the noise is also removed unit time after the unit time and this complete process is systematic.
    noise is added                                      noise is removed 
    forward stable diffusion                            reverse stable diffusion
    conversion of pixels into the embeddings            conversion of the embeddings to the pixels
    Encoding                                            Decoding




Text Generation
    chatgpt 3.5/4, bard
Image Generation
    Dalle-E3, Stable diffusion
Music Generation
    Music-LM
Video Generation
    Runway ML, Gen-2

Prompt Engineering:
A prompt refers to the input or instructions provided to the model to generate a specific response or output.
Anatomy of Prompt Engineering
1. Simulate Persona 
2. Taks
3. Steps to complete Tasks
4. Context/Constraints
5. Goal
6. Format Output

Tips for Effective Prompt Engineering:
be explicit: clearly specify the desired format or output.
provide context: offer relevant information or background to guide the model.
control output length: instruct the model to generate responses of a specific length.
restrict responses: utilize techniques like temperature adjustments to refine output quality.
experiment and iterate: refine prompts through experimentation and feedback loops.

Servers important parts:
hard disk 
ram
processer
network 
these factors decide the performance of the server being in use.

API (Application programming interface)
API acts as an intermediate or we can say the medium between two channels for the effective communication.
like we use the open ai  api keys for the differnent type of models execution like for the LLM related models.
API keys are very sensitive. It is very important to keep them in secret. we keep them in the secret for saving
ourself from the execessive cost expenditures.
openai is not free it charges specific smount of money but the hugging face can provide almost similar sort of 
functionalities free of cost.

pipeline:
pipeline is a terminology in the data science which is used to represent all the processes like 
model training 
model prediction 
visual presentation of the prediction

pipelines are already built like trained models present in the hugging face.

name entity recognition:
recognition of the named entity from the text is called teh name entity recognition. an important step is the text summarization.
also used for the blind people in the glasses, driverless cars 
In image classification moels give the top 5 labels which are called top 5 accuracy 

pytorch
used for the matrix/tensors multiplication operations that can run parallelely.
difference between cpu and gpus:
suppose you have the problem to find the sum of two numbers for 1000 different people and in cpu there are around 8 or 16 or 32 cores/processors 
which are capacity/ability wise very much capable enough on the other hand gpu has 2000 cores/processors (a very simple one )
but they will solve the problem faster then the cpu because the resources are more as comapred to the cpu like for eample 
for the same problem you have 1000 matric students and 8 phd teachers so, who will solve it faster? obviously matric students because they can 
performance the task parallely and phd teacher will do it step wise although they are more capable.

exactly the same way for using the gpu we need the resources system wise and for openai we need the cost.
coda:
coda is the medium or translater of the pytorch that is built only for the nvidia gpus for the translation of the specifi task fro the translation 
between the processor and the model.
like trnaslation between two countrymen individuals we need the translater that translater knows only the language of two specific countries now if the 
third country comes in the translater gets failed.
similarly coda gets failed in the translation if the gpu other then nvidia comes in.
 
data  science:
extracting knowledge and insights by finding the patterns from the data.
data science processes:
1. acquire
2. prepare (eda) 
3. analyze
4. report
single attribute
mean    -------->  measure of dispersion ----------------> variance , std.deviation
median  ---------> measure of dispersion ---------------> quartiles , inter-quartile range
more then single attributes
measure of proximity --------------> dot product, cross product etc 

symmetric
asymmetric (positively skewed , negatively skewed)


techniques for the dimensionality reductiomn:
1. pca (principal component analysis)
it uses linear algebra techniques 
igenty composition
singular value decomposition
2. t- stochastic neighbor embedding

techniques to be applied on the data:
aggregation (aggregating the aspects from the data)
smoothing (arranging the data into the particular or clean pattern)
discretization (converting the data into the categories)
feature extraction (extracting meaningful feature from the existing ones)
data transfromation (normalization)

normalization types
z score / standard  (mean and std.deviation application)
minmax (min max value from data dealing with remaining values)
decimal scaling (dealing with point values in the floating numbers)


bagging and boosting

regression analysing
mse(mean squared error) 
rmsa (root mean squared analysis)
residual analysis (analysing overfitting and underestimating)

classification analysing
confusion matrix
precesion
recall
accuracy
f1-score 

machine learning techniques:
    classification techniques:
        1. logistic regression
            it is the binary classification technique like it predicts the data into two categories like 0 or 1 or we can say a cat or non-cat. its name is confusing like the logistic
            regression because it involves the numeric values involves into it like this technique uses the numeric value to represent the nominal data for instance predicting whether
            it is cat or non cat by mapping it with the values like 0 and 1. if it is cat it will be in the category of 1 and if it is non cat it will be in the category 0. In regression,
            the numeric values ranges from -inf to +inf. but in the logistic regression the values ranges from 0 to 1. Actually there involves the activation function (sigmoid function) that
            coverts the nominal data into the binary values for the model to carry out its specific functionalities.
        2. decision tree classifier
            decision tree classifier applies better on the structured data the data is structured in the form of tree based on the features or attributes present in the data available.
            a main featur/attribute is kept at the root node the feature which can categorize the data into two main categories moving towards the purity of the data. tree involves the
            nodes and leaves nodes represent the features and the leaves represents the decision being made. decision is made on each node to categorize the data further and moving 
            towards the main decision.
        3. random forest classifier
            random forest classifier is also one of the technique which is based on the decision tree classifier.multiple decision trees are being used in the random forest to move towards 
            the main decision to be predicted. actually all the decision trees are being executed parallely and the single decision is being carried out in the end. it processes like the 
            vooting process the output which have more dominance or more existence is the result in the end.
        4. grading boosting classifier
            this technique also involves the decision trees execution in it but they are carried out sequentially like step by step. in the end the result involves the performance of all the 
            trees if one tree has made some mistakes then the next tree removes the mistakes from the previous tree and this process gets improved and improved and hence the process is very 
            significant. it is most efficient technique for the classification.

    regression techniques:
    1. linear regression
        linear regression is first technique for the regression as it predicts the numeric values on the base of given numeric values. basically a graph is plotted using the given 
        o predict the given value numeric values and then it i tried to plot the regression line where the maximum data points exist. that line is called in mathematics y = mx+c.
        this line helps then a function is made in mathematics f(x)= wx+b this function helps us to find the value. actually the cost/loss function is used to predict the value by 
        fine tunning  actually the w is being put into the function then the answer of wx is extracted and then the difference between the predicted and actual value is extracted 
        more is the difference means the value being put into is wrong and needs to be changed so value is changed and this process is being carried out in the iterations called the 
        fine tunning of the model.
            Types of regression line:
                linear regression line
                polynomial regression line
            Types of regression model:
                univariant regression
                    if the regression model is made based on the single feature/attribute/variable.
                multivariet regression
                    if the regression model is made based on the multiple features/attributes/variables.
    2. decision tree regressor
        decision tree regressor is also one of the regression technique each node categorize the data into the numeric ranges then the leaves of the trees represents the average score 
        or the final predicted score.
    3. random forest regressors
        random forest regressors involves the decision trees being carrying out their functionalities in a parallel fashion (like the vooting process). 
    4. grading boosting regressors
        grading boosting regressors involves the decision trees functionalities being carried out in a sequence fashion.
evaluation of the model:
residual analysis (the difference between the actual value and predicted value)
root squared error
root mean squared error

Evaluation metrics
    point metrics
    summary metrics
accuracy
precision (it is the evaluation metrics that determines that how many predicted values are right from the overall predicted values)
recall (it is the evaluation metrics that determines that how many predicted values are right from the actual values)
    true positive rate
    tru negative rate 
confusion metrics
roc - auc 


deep learning
short comings / limitations of the conventional machine learning 
manual feature extraction
cannot deal with the unstructured data or cannot deal with the unstructured data better
cannot deal with high dimensional data

deep learning is the non-linear model 
it involves the neural networks inside it also involves the two main processes in it forward propogation and the second one is the reverse/backward propogation.
prediction is made in the forward propgation while the training is being carried out in the backward propogation.








