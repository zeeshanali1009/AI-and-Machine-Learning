Artifical Intelligence:
    1. ANI (Artificial Narrow Intelligence)
        like smart speakers, self driving cars, web search, AI in farming and factories.
    2. AGI (Artificial General Intelligence)
        do anything that human can do.

 Artifical Intelligence
 It is the branch of computer science concerned with development of methods that allows computer to learn without explicit programming.
 Search algorithms, rule based system, ststistical inference, machine learning.
    Machine Learning
    It is the branch of artificial intelligence that can learn from examples and experience instead of relying on hard-coded rules and make predictions on new data.
    svm, tree algorithms, nearest neighbors, bagging, boosting, deep learning.
        Deep Learning
        It is the subfield of machine learning that focuses on neural networks (inspired from biological neurons) to deveelop learning models.
        fcns, cnns, rnns, transformers, autoencoders, gans
------------------------------------------------------------------------------------------------------------------------------------------------------------
Samples and datapoints are the same terminologies.
Neuron:
Neuron is the most fundamntal part of the deep learning which contains the three basic things: input interconnections and the output.
Neuron is basically a function that performs a specific function.
Neurons need great:
data
computational power
well trained algorithms

Artificial intelligence is distributed into two parts
1. Descriminative (Creates the patterns, descriminate the data, make the seperations, involes the conventional models that deals with structured data better)
    1. Supervised Learning
    2. Unsupervised learning
    3. Reinforcement learning
2. Generative (generates the information can deal with both structured and unstructured data as well)
    1. NLP(Natural Language Processing)
        It deals with the natural language with the text and with the neurons that deals with this sort of data.
    2. Speech Recognition
        It deals with the speech,voice related data.
    3. Computer Vision
        it deals with the images, videoes.

Generative AI Models:
1. LLM
    LLMs are the generative pretrained transformers that are too large such as chatgpt 3 had 176 billion parameters/neurons.
    Tokenization is the most important concept in the NLP which is the process of breaking down the tecxt into the smaller units called the tokens. which 
    can be words, phrases even the chracters. like it is the vocabulary of the models. token can be single chracter, word or the sentence.
    like: This is the first step in NLP has the tokens t h i s i s t h e f i r s t .........
    context window: it is the limit of the model to accept , process, the tokens like gpt has the 4k and so on etc.
2. Latent Diffusion Models
    Diffusion means going from high intensity towards the lower intensity (In generic) like the ink drop being fallen on the shirt and then starts to spread slowly.
    Stable diffusion
    Latent diffusion model (latent means hidden)
    Actually when we say that the picture is 1024 x 1024 it means that there are many pixels in it (like in millions) one individual pixel has three colors rgb so there is too much 
    computation required for the images in the ai field then for the minimization of the computation we use the latent vectors behind that reduces the computation power to 48 times.
    hence the model is called the diffusion model.
   
    Actually the noise is being added into the picture unite time after unit time and then the noise is also removed unit time after the unit time and this complete process is systematic.
    noise is added                                      noise is removed 
    forward stable diffusion                            reverse stable diffusion
    conversion of pixels into the embeddings            conversion of the embeddings to the pixels
    Encoding                                            Decoding




Text Generation
    chatgpt 3.5/4, bard
Image Generation
    Dalle-E3, Stable diffusion
Music Generation
    Music-LM
Video Generation
    Runway ML, Gen-2

Prompt Engineering:
A prompt refers to the input or instructions provided to the model to generate a specific response or output.
Anatomy of Prompt Engineering
1. Simulate Persona 
2. Taks
3. Steps to complete Tasks
4. Context/Constraints
5. Goal
6. Format Output

Tips for Effective Prompt Engineering:
be explicit: clearly specify the desired format or output.
provide context: offer relevant information or background to guide the model.
control output length: instruct the model to generate responses of a specific length.
restrict responses: utilize techniques like temperature adjustments to refine output quality.
experiment and iterate: refine prompts through experimentation and feedback loops.

Servers important parts:
hard disk 
ram
processer
network 
these factors decide the performance of the server being in use.

API (Application programming interface)
API acts as an intermediate or we can say the medium between two channels for the effective communication.
like we use the open ai  api keys for the differnent type of models execution like for the LLM related models.
API keys are very sensitive. It is very important to keep them in secret. we keep them in the secret for saving
ourself from the execessive cost expenditures.
openai is not free it charges specific smount of money but the hugging face can provide almost similar sort of 
functionalities free of cost.

pipeline:
pipeline is a terminology in the data science which is used to represent all the processes like 
model training 
model prediction 
visual presentation of the prediction

pipelines are already built like trained models present in the hugging face.

name entity recognition:
recognition of the named entity from the text is called teh name entity recognition. an important step is the text summarization.
also used for the blind people in the glasses, driverless cars 
In image classification moels give the top 5 labels which are called top 5 accuracy 

pytorch
used for the matrix/tensors multiplication operations that can run parallelely.
difference between cpu and gpus:
suppose you have the problem to find the sum of two numbers for 1000 different people and in cpu there are around 8 or 16 or 32 cores/processors 
which are capacity/ability wise very much capable enough on the other hand gpu has 2000 cores/processors (a very simple one )
but they will solve the problem faster then the cpu because the resources are more as comapred to the cpu like for eample 
for the same problem you have 1000 matric students and 8 phd teachers so, who will solve it faster? obviously matric students because they can 
performance the task parallely and phd teacher will do it step wise although they are more capable.

exactly the same way for using the gpu we need the resources system wise and for openai we need the cost.
coda:
coda is the medium or translater of the pytorch that is built only for the nvidia gpus for the translation of the specifi task fro the translation 
between the processor and the model.
like trnaslation between two countrymen individuals we need the translater that translater knows only the language of two specific countries now if the 
third country comes in the translater gets failed.
similarly coda gets failed in the translation if the gpu other then nvidia comes in.
 
data  science:
extracting knowledge and insights by finding the patterns from the data.
data science processes:
1. acquire
2. prepare (eda) 
3. analyze
4. report
single attribute
mean    -------->  measure of dispersion ----------------> variance , std.deviation
median  ---------> measure of dispersion ---------------> quartiles , inter-quartile range
more then single attributes
measure of proximity --------------> dot product, cross product etc 

symmetric
asymmetric (positively skewed , negatively skewed)


techniques for the dimensionality reductiomn:
1. pca (principal component analysis)
it uses linear algebra techniques 
igenty composition
singular value decomposition
2. t- stochastic neighbor embedding





