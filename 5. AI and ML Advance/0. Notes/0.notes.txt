Artifical Intelligence:
    1. ANI (Artificial Narrow Intelligence)
        like smart speakers, self driving cars, web search, AI in farming and factories.
    2. AGI (Artificial General Intelligence)
        do anything that human can do.

 Artifical Intelligence
 It is the branch of computer science concerned with development of methods that allows computer to learn without explicit programming.
 Search algorithms, rule based system, ststistical inference, machine learning.
    Machine Learning
    It is the branch of artificial intelligence that can learn from examples and experience instead of relying on hard-coded rules and make predictions on new data.
    svm, tree algorithms, nearest neighbors, bagging, boosting, deep learning.
        Deep Learning
        It is the subfield of machine learning that focuses on neural networks (inspired from biological neurons) to deveelop learning models.
        fcns, cnns, rnns, transformers, autoencoders, gans
------------------------------------------------------------------------------------------------------------------------------------------------------------
Samples and datapoints are the same terminologies.
Neuron:
Neuron is the most fundamntal part of the deep learning which contains the three basic things: input interconnections and the output.
Neuron is basically a function that performs a specific function.
Neurons need great:
data
computational power
well trained algorithms

Artificial intelligence is distributed into two parts
1. Descriminative (Creates the patterns, descriminate the data, make the seperations, involes the conventional models that deals with structured data better)
    1. Supervised Learning
    2. Unsupervised learning
    3. Reinforcement learning
2. Generative (generates the information can deal with both structured and unstructured data as well)
    1. NLP(Natural Language Processing)
        It deals with the natural language with the text and with the neurons that deals with this sort of data.
    2. Speech Recognition
        It deals with the speech,voice related data.
    3. Computer Vision
        it deals with the images, videoes.

Generative AI Models:
1. LLM
    LLMs are the generative pretrained transformers that are too large such as chatgpt 3 had 176 billion parameters/neurons.
    Tokenization is the most important concept in the NLP which is the process of breaking down the tecxt into the smaller units called the tokens. which 
    can be words, phrases even the chracters. like it is the vocabulary of the models. token can be single chracter, word or the sentence.
    like: This is the first step in NLP has the tokens t h i s i s t h e f i r s t .........
    context window: it is the limit of the model to accept , process, the tokens like gpt has the 4k and so on etc.
2. Latent Diffusion Models
    Diffusion means going from high intensity towards the lower intensity (In generic) like the ink drop being fallen on the shirt and then starts to spread slowly.
    Stable diffusion
    Latent diffusion model (latent means hidden)
    Actually when we say that the picture is 1024 x 1024 it means that there are many pixels in it (like in millions) one individual pixel has three colors rgb so there is too much 
    computation required for the images in the ai field then for the minimization of the computation we use the latent vectors behind that reduces the computation power to 48 times.
    hence the model is called the diffusion model.
   
    Actually the noise is being added into the picture unite time after unit time and then the noise is also removed unit time after the unit time and this complete process is systematic.
    noise is added                                      noise is removed 
    forward stable diffusion                            reverse stable diffusion
    conversion of pixels into the embeddings            conversion of the embeddings to the pixels
    Encoding                                            Decoding




Text Generation
    chatgpt 3.5/4, bard
Image Generation
    Dalle-E3, Stable diffusion
Music Generation
    Music-LM
Video Generation
    Runway ML, Gen-2

Prompt Engineering:
A prompt refers to the input or instructions provided to the model to generate a specific response or output.
Anatomy of Prompt Engineering
1. Simulate Persona 
2. Taks
3. Steps to complete Tasks
4. Context/Constraints
5. Goal
6. Format Output

Tips for Effective Prompt Engineering:
be explicit: clearly specify the desired format or output.
provide context: offer relevant information or background to guide the model.
control output length: instruct the model to generate responses of a specific length.
restrict responses: utilize techniques like temperature adjustments to refine output quality.
experiment and iterate: refine prompts through experimentation and feedback loops.

Servers important parts:
hard disk 
ram
processer
network 
these factors decide the performance of the server being in use.

API (Application programming interface)
API acts as an intermediate or we can say the medium between two channels for the effective communication.
like we use the open ai  api keys for the differnent type of models execution like for the LLM related models.
API keys are very sensitive. It is very important to keep them in secret. we keep them in the secret for saving
ourself from the execessive cost expenditures.
openai is not free it charges specific smount of money but the hugging face can provide almost similar sort of 
functionalities free of cost.

pipeline:
pipeline is a terminology in the data science which is used to represent all the processes like 
model training 
model prediction 
visual presentation of the prediction

pipelines are already built like trained models present in the hugging face.

name entity recognition:
recognition of the named entity from the text is called teh name entity recognition. an important step is the text summarization.
also used for the blind people in the glasses, driverless cars 
In image classification moels give the top 5 labels which are called top 5 accuracy 

pytorch
used for the matrix/tensors multiplication operations that can run parallelely.
difference between cpu and gpus:
suppose you have the problem to find the sum of two numbers for 1000 different people and in cpu there are around 8 or 16 or 32 cores/processors 
which are capacity/ability wise very much capable enough on the other hand gpu has 2000 cores/processors (a very simple one )
but they will solve the problem faster then the cpu because the resources are more as comapred to the cpu like for eample 
for the same problem you have 1000 matric students and 8 phd teachers so, who will solve it faster? obviously matric students because they can 
performance the task parallely and phd teacher will do it step wise although they are more capable.

exactly the same way for using the gpu we need the resources system wise and for openai we need the cost.
coda:
coda is the medium or translater of the pytorch that is built only for the nvidia gpus for the translation of the specifi task fro the translation 
between the processor and the model.
like trnaslation between two countrymen individuals we need the translater that translater knows only the language of two specific countries now if the 
third country comes in the translater gets failed.
similarly coda gets failed in the translation if the gpu other then nvidia comes in.
 
data  science:
extracting knowledge and insights by finding the patterns from the data.
data science processes:
1. acquire
2. prepare (eda) 
3. analyze
4. report
single attribute
mean    -------->  measure of dispersion ----------------> variance , std.deviation
median  ---------> measure of dispersion ---------------> quartiles , inter-quartile range
more then single attributes
measure of proximity --------------> dot product, cross product etc 

symmetric
asymmetric (positively skewed , negatively skewed)


techniques for the dimensionality reductiomn:
1. pca (principal component analysis)
it uses linear algebra techniques 
igenty composition
singular value decomposition
2. t- stochastic neighbor embedding

techniques to be applied on the data:
aggregation (aggregating the aspects from the data)
smoothing (arranging the data into the particular or clean pattern)
discretization (converting the data into the categories)
feature extraction (extracting meaningful feature from the existing ones)
data transfromation (normalization)

normalization types
z score / standard  (mean and std.deviation application)
minmax (min max value from data dealing with remaining values)
decimal scaling (dealing with point values in the floating numbers)


bagging and boosting

regression analysing
mse(mean squared error) 
rmsa (root mean squared analysis)
residual analysis (analysing overfitting and underestimating)

classification analysing
confusion matrix
precesion
recall
accuracy
f1-score 

machine learning techniques:
    classification techniques:
        1. logistic regression
            it is the binary classification technique like it predicts the data into two categories like 0 or 1 or we can say a cat or non-cat. its name is confusing like the logistic
            regression because it involves the numeric values involves into it like this technique uses the numeric value to represent the nominal data for instance predicting whether
            it is cat or non cat by mapping it with the values like 0 and 1. if it is cat it will be in the category of 1 and if it is non cat it will be in the category 0. In regression,
            the numeric values ranges from -inf to +inf. but in the logistic regression the values ranges from 0 to 1. Actually there involves the activation function (sigmoid function) that
            coverts the nominal data into the binary values for the model to carry out its specific functionalities.
        2. decision tree classifier
            decision tree classifier applies better on the structured data the data is structured in the form of tree based on the features or attributes present in the data available.
            a main featur/attribute is kept at the root node the feature which can categorize the data into two main categories moving towards the purity of the data. tree involves the
            nodes and leaves nodes represent the features and the leaves represents the decision being made. decision is made on each node to categorize the data further and moving 
            towards the main decision.
        3. random forest classifier
            random forest classifier is also one of the technique which is based on the decision tree classifier.multiple decision trees are being used in the random forest to move towards 
            the main decision to be predicted. actually all the decision trees are being executed parallely and the single decision is being carried out in the end. it processes like the 
            vooting process the output which have more dominance or more existence is the result in the end.
        4. grading boosting classifier
            this technique also involves the decision trees execution in it but they are carried out sequentially like step by step. in the end the result involves the performance of all the 
            trees if one tree has made some mistakes then the next tree removes the mistakes from the previous tree and this process gets improved and improved and hence the process is very 
            significant. it is most efficient technique for the classification.

    regression techniques:
    1. linear regression
        linear regression is first technique for the regression as it predicts the numeric values on the base of given numeric values. basically a graph is plotted using the given 
        o predict the given value numeric values and then it i tried to plot the regression line where the maximum data points exist. that line is called in mathematics y = mx+c.
        this line helps then a function is made in mathematics f(x)= wx+b this function helps us to find the value. actually the cost/loss function is used to predict the value by 
        fine tunning  actually the w is being put into the function then the answer of wx is extracted and then the difference between the predicted and actual value is extracted 
        more is the difference means the value being put into is wrong and needs to be changed so value is changed and this process is being carried out in the iterations called the 
        fine tunning of the model.
            Types of regression line:
                linear regression line
                polynomial regression line
            Types of regression model:
                univariant regression
                    if the regression model is made based on the single feature/attribute/variable.
                multivariet regression
                    if the regression model is made based on the multiple features/attributes/variables.
    2. decision tree regressor
        decision tree regressor is also one of the regression technique each node categorize the data into the numeric ranges then the leaves of the trees represents the average score 
        or the final predicted score.
    3. random forest regressors
        random forest regressors involves the decision trees being carrying out their functionalities in a parallel fashion (like the vooting process). 
    4. grading boosting regressors
        grading boosting regressors involves the decision trees functionalities being carried out in a sequence fashion.
evaluation of the model:
residual analysis (the difference between the actual value and predicted value)
root squared error
root mean squared error

Evaluation metrics
    point metrics
    summary metrics
accuracy
precision (it is the evaluation metrics that determines that how many predicted values are right from the overall predicted values)
recall (it is the evaluation metrics that determines that how many predicted values are right from the actual values)
    true positive rate
    tru negative rate 
confusion metrics
roc - auc 


deep learning
-short comings / limitations of the conventional machine learning 
-manual feature extraction
-cannot deal with the unstructured data or cannot deal with the unstructured data better
-cannot deal with high dimensional data

deep learning is the non-linear model 
it involves the neural networks inside it also involves the two main processes in it forward propogation and the second one is the reverse/backward propogation.
prediction is made in the forward propgation while the training is being carried out in the backward propogation.

parameters and hyper parameters
parameters are the features that is learned from the data by the model
hyper parameters are controlled by the experience of the developer they totally controlled by the developer but have direct effect on the models.


1. fully connected neural network fcn, standard neural network , artificial neural network , dense neural network 
    limitation:
        increased size of biases and weights in the calculation process (extensive compute required for forward pass and reverse pass )
        limited feature extraction 
2. convolutional neural network
    concept:
        cnns are used for the pictures images specifically.
        actually the convolution is applied, in the convolution a specific sized filter is applied on the image matrix thus giving the feature map this process is called the 
        covoution. the inner converted from is called the latent,bottleneck or latent represention and then this feature map is the main output then the pooling is applied 
        (summary process ). then there is the fully connected neural network and the output which is called the head.
        pooling (it is the process of getting the most important features from the given dataset)
        padding (adding the extra layers to make the feature extraction process more accurate)
        strides (way of running the filters on the input image matrix)
       covolutionalneural network 1 d can also work on the time seeies data.
3. Auto encoders
     concept
        it involves the two parts wncoder and decoder actually it makes the structural summary of the data that can be 
        reverse ro the previous original position also.
        Auto encoder is the identity dunction actually (the function which gives the same output as the input was provided).
        autoencoder can work also on the unlabelled data.
        application of the autoencoders:
        Types of autoencoders:
        1. vanilla autoencoder
        image compression, anomaly detection ,data denoising ,dimensionality reduction
        2. denoising autoencoder
        image denoising, audio denoising etc
        3. Convolutional autoencoders
        4. variational autoencoder 
        generative ai
        

        regularization like dropout example 

        loss monitoring
        restruction loss 
        kl divergence loss
 
generalization:
if the model gives the correct data on the unseen data as well so this concept is called the generaLization.

RNN (Recurrent neural network)
these neural networks work upon the time series data or the structured data. in rnn the most important role is of the neurnon that stores the information about the context
types of rnns (actually there is the storage process of the context more long is the context storage more reliable it becomes so to resolve this problem we have two types of the rnn ) 
1. grn  (gateway recurrent network) a specific gateway is introduces to reduces the information to be stored in the context
2. lstm (long short term memory) it sotores the short term memory for the long time thus improves the context as well.

Attention models (seq2seq models):
they takes only the important data summerical data.
types of attention models are self attention and multi-head attention(have the more heds to understand the context in the llm)

variable sequence:
there exists always the variable sequence which is detected by the llms usually n-gram helps to detect the variable sequence. it adds the paddiding in the sequence like it adds the empty places in the given sequence.

embedding vectors:
actually the embedding vectors are the representation in the form of numbers of the raw data.

Transformers uses the self attentions.
certain input -------------------- certain output
encoder block -------------------- decoder block
positional Encoding----------------- masking
assigns the numbers to
the text for the embeddings
purposes

there exists some problems like 
information overload era
less information about the certain context 
so these above problems are solves by the rag or by fine tunning
rag (raw augmented generation)
langchain acts as the bridge bw nlp and dataset.
rag pipeline:
ingestion                                                   retrieval                               synthesis
document procesing into the chunks                          query-index-response                    response


vector stores:
faiss, etc
these are the stores to store the embeddings or the vector form of the data
vector stores that deals with the words better but not deal with the query better
svm     matching the words and maintains the longest possible context by reducing the matching words upto one
tf/ide  take the most repeated words or the diversed data only
finding vectors mechanism
cosine similarity 
dot product 
mml (maximum marginal retrieval/relevance)

retriever
it is the interface that acts as the medium for dealing with the content of the vector store.

chains:
involes the following actions
stream invoke branch
runnable 
runable parrallel
prompt template  (few short prompt template, chat prompt template )
itemgetter
python function that is used to get the specific key's value from the specific dictionary.
output parser
it actually specifies the format of the output to be dispalyed.












